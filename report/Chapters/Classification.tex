%----------------------------------------------------------------------------------------
%	CLASSIFICATION OF DATA VIA DIFFERENT METHODS
%
%	Classification of the data using different approaches.
%----------------------------------------------------------------------------------------
\section{Classification}

Given the structure of our dataset, namely image data with a sub dataset with corresponding class labels, and a sub dataset with corresponding feature labels, different machine learning and computer vision methods were chosen, to tackle the problem of image classification. \\
\\
Image classification refers to the method of identifying to which category an image belongs to, according to its visual information.  Classification problems can be divided into three different types: binary, multi-class and multi-label. Whereas a binary classification only distinguishes between two different classes and therefore classifies an image into one of the two classes, a multi-class classification distinguishes between multiple exclusive classes. A multi-label classification also works for multiple classes. However in the latter, a single image can belong to none, one, several or all of the classes. Those classes can be seen as a feature vector for each image. Each feature can be present or not, independently of the other features. ( vielleicht hier zitieren, seite muss noch rausgesucht werden~\citep{har2003constraint}) \\
\\
Each of those classification methods comes with certain advantages and implications.
There exist many classification methods which have been developed for binary classification problems, but less methods are suited for multi-label or multi-class classification. Therefore, the latter often work as a combination of binary classifiers. Moreover, multi-class and multi-label classification have the difficulty of sparser labels.  \\
\\
Binary classification can be used to decide whether a certain feature is present at one certain asparagus piece, or not. This is helpful for a first inspection of the data, but does not enable a full classification of one image into one of 13 classes, which are currently sorted at the Spargelhof Gut Hosterfeld. Multi-class classification solves this problem. It is fairly easy to apply this classification type on the pre labelled images, but increasingly difficult for the semi-supervised and unsupervised approaches. While it enables a clear identification of class belonging, it does not enable to train variability within classes. As the class id results from a combination of the presence of certain features, and the absence of others, it is therefore also reasonable to go for a multi-label classification approach.  \\
\\
Moreover, there are different methods on how to approach image classification. Those can be divided into three main groups: supervised learning, semi supervised learning and unsupervised learning. In addition to classical computer vision-based approaches, our study group investigated several neural network approaches. \\
\\
During our group work, algorithms of all three different classification types (binary, multiclass and multilabel) as well as of all three learning types (supervised, semi-supervised and unsupervised) were applied for different working steps and different purposes. \\
\\
In the long run, an integrated model was aimed which predicts all features of a single asparagus piece, and from which an additional class can be inferred. However, as intermediate steps towards that goal, the focus was to optimize models on identifying the presence of single features. Besides that, we only investigated a few multi-label classification tasks.   \\
\\
The following chapter aims to give a general background of the different approaches chosen for our image classification problem, as well as a detailed overview of the concrete implementations of the models and the mechanisms of their hyperparameters. 
All algorithms were implemented using Python.  


\subsection{Supervised learning}

In machine learning, there are different approaches for an application to be trained on a set of data~\citep{geron2019hands}~\citep{bishop2006pattern}. Depending on the level of supervision that the system receives during the training phase, the learning process is grouped into one of four major categories~\citep{geron2019hands}. One of these categories is supervised learning. For supervised learning approaches, the training data includes not only the input but also its corresponding target labels, which is why only labelled data is used. The objective is to find a mapping between target (x) and label (y) when a set of both (x,y) is provided as training data to the application~\citep{olivier2006semi}. An advantage of supervised learning is that the problem is well defined and the model can be evaluated in respect to its performance on a labelled data set~\citep{daume2012course}~\citep{olivier2006semi}. In other words, the labels can be used as a direct basis for the model optimizing function during training. \\
Supervised learning spans over a large set of different methods, from decision trees and random forests, to support vector machines (SVM), to neural networks~\citep{caruana2006comparison}~\citep{geron2019hands}. \\
A classic task for supervised learning systems is the classification of received data and mapping it to one of a finite number of categories~\citep{bishop2006pattern}.
The disadvantage of supervised learning is the effort of receiving enough labelled data. It can be challenging to obtain fully labelled data because labelling experts are needed to classify the data which is usually a time consuming and expensive task~\citep{zhu05survey}~\citep{figueroa2012predicting}. \\
\\
In the following subchapters, different supervised learning methods were chosen to solve the classification task either (a) by using the labelled data that we collected by running pre-sorted samples a second time through the sorting machine, or (b) by using the data that was manually labelled for features as described in chapter 3. In  4.1.1 \textit{Prediction based on feature engineering}, an approach using a multi-layer perceptron (MLP) is described for feature classification. In the second section 4.1.2 \textit{A dedicated network for head-related features}, a convolutional neural network is used to train solely on the head image data for the features flower and rusty head. The third section, 4.1.3 \textit{Single-label classification}, is concerned with labelling the input images with a convolutional neural network (CNN) in a binary setup for their designated features. In 4.1.4 \textit{Multi-label classification}, a neural network is trained on the data to label it not only for one feature but all features at the same time.
In the last section 4.1.5 \textit{From feature to label}, a random forest approach is described to map the features of the image data to their class label.



\subsubsection{Prediction based on feature engineering}

Besides approaches that directly use images as an input one may use high level feature engineering to retrieve sparse representations that contain relevant information in a condensed form and apply classical machine learning classifiers such as MLPs to predict labels~\citep{zheng2018feature}. These classifiers are comparatively simple, fast to train and only few network hyperparameters have to be defined. One may argue that this is one of the major benefits as compared to networks of higher complexity (e.g. convolutional deep learning nets) that are typically characterized by a more complex structure and hence a larger hyperparameter space. As a consequence, finding suitable parameters for MLPs (i.e. the number of hidden layers and neurons per layer) is comparatively easy(Footnote: The challenge of finding appropriate network parameters is well known in the deep learning community: “Designing and training a network using backprop requires making many seemingly arbitrary choices [...] These choices can be critical, yet there is no foolproof recipe for deciding them because they are largely problem and data dependent”~\citep{lecun2012efficient}. The requirement to specify hyperparameters is a disadvantage of neural networks (including MLPs) as compared to parameter free methods (ScikitLearn \url{https://scikit-learn.org/stable/modules/neural\underscore networks\underscore supervised.html}). Due to the combinatorial explosion the above mentioned challenge of finding suitable is harder for more complex networks as more options must be considered.): An extensive search in hyperparameter space is practicable because of its small size and because training on sparse representations limits the number of neurons in the networks which results in fast training that allows for many experiments. As compared to convolutional networks for deep learning where suitable means to avoid the vanishing gradient problem are required (\url{https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484}), kernel sizes, strides, the number of kernels and other parameters must be defined there simply less one could do wrong when defining shallow MLPs. It deserves no further explanation that underfitting can potentially be due to an unsuitable network design or because predictions are impossible due to incongruencies or missing information in the sparse data set~\citep{lecun2012efficient}. If the learning task is simple enough to be accomplished by MLPs (e.g. finding combinations of partial angles that correspond to the impression of curvature), one may hence speculate that underfitting can rather be explained by incongruencies or missing information in the labels then a result of issues in design and training hyperparameters of the network. This classical machine learning approach that relies on feature engineering was applied to predict features based on colour and partial angles of asparagus spears.
\\
\\
\textbf{Violet and rust prediction based on color histograms} \\
The initial approach of measuring the feature “violet” that is based on distribution of sufficiently intense colour hues in the violet range faces at least two drawbacks: First it requires to define two thresholds and second the impression of a violet asparagus piece could potentially be affected by the mix of colours (combinations of colours) that are potentially outside the violet range or are too pale to be considered (see XXX). The same holds for the feature “rust”. Hence in a second approach histograms were computed for foreground pixels after transforming the images to palette images with 256 colour hues (see XXX). The resulting representation is arguably a sparse descriptor that allows to predict colour features using explicitly defined rules or trainable machine learning models. \\
\\
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{Figures/chapter04/feature_engineering}
	\decoRule
	\caption[??]{??}
	\label{fig:FeatureEngineering}
\end{figure}

\\
\begin{tabular}{lrrrrrr}\n\\toprule\n{} &  False positive &  False negative &  True positive &  True negative &  Sensitivity &  Specificity \\\\\n\\midrule\nis\\violet     &            0.04 &            0.03 &           0.05 &           0.88 &         0.62 &         0.96 \\\\\nhas\\rost\\body &            0.19 &            0.14 &           0.33 &           0.34 &         0.71 &         0.65 \\\\\n
\bottomrule\n
\end{tabular}\n


A simple MLP with four hidden layers and 128 neurons in each of them was trained on the resulting normalized histograms of palette colours (ReLU activation / sigmoid activation in the final layer). Hyperparameters were optimized and the network was trained for a total of 500 epochs as the learning curve indicated convergence at this point. \\
\\
\textbf{Curvature prediction based on partial angles} \\
Although the accuracies are far from perfect the results appear to be promising. The hit rate for curvature detection is high: 82\% of all bended pieces were identified as such. In comparison, this holds for 71\% of the pieces affected by rust and 62\% of the violet pieces. In contrast, almost all pieces that are identified not to be violet are labelled accordingly (96\% specificity) whereas the specificity for rust (65\%) and curvature (67\%) is lower. The receiver operating characteristic reveals that the prediction is of better quality for violet and curvature prediction as compared to rust prediction which is reflected in a smaller area under the curve. Possibly this reflects that rather small brown spots were considered rust by some coders that attributed labels.
 \\

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{Figures/chapter04/fe_curve}
	\decoRule
	\caption[??]{??}
	\label{fig:FeatureEngineeringCurve}
\end{figure}

\begin{tabular}{lrrrrrr}\n\\toprule\n{} &  False positive &  False negative &  True positive &  True negative &  Sensitivity &  Specificity \\\\\n\\midrule\nis\\bended &            0.19 &            0.07 &           0.34 &            0.4 &         0.82 &         0.67 \\\\\n\bottomrule\n
\end{tabular}\n

Considering the low agreement in labeling, higher values for the specificity and sensitivity of the classifier were not expected. A likely explanation is that the model generalizes deviating understandings and incongruencies we had when attributing labels and affected the reliability of the data. Arguably only little information was discarded by computing the sparse representations that served as an input. Information about irregularities in the outline that a center line does not reflect but potentially contribute to the perception of curvature are not reflected in named sparse representation. The same holds for the spatial distribution of coloured pixels might contain additional information regarding rust and violet-detection. However, the major criteria are captured. As MLPs have little hyperparameters that are suitable for non-linear mappings and  as the task of mapping high level features to human estimates appears to be rather simple, one could argue that there is little potential to improve the predictive quality using other techniques. By introducing a bias, the sensitivity (true positive rate) of the classifier can be adjusted at the cost of more false positives. Introducing a bias means that the threshold that is used to convert the floating point outputs of a neural network to booleans that indicate whether a feature is present or not can be set to values other than 0.5. The possibility of making the classifier more or less sensitive appears to be a good option to be implemented as a feature for customization by the user in asparagus sorting machines. The resulting behavior is reflected in the receiver operating characteristic that is calculated using different biases.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{Figures/chapter04/fe_roc}
	\decoRule
	\caption[??]{??}
	\label{fig:FeatureEngineeringROC}
\end{figure}



\subsubsection{A dedicated network for head-related features}

Some features relate to the asparagus heads only. Hence, it was assumed that classification is easiest when training a convolutional neural network on depictions of the respective region of the asparagus. Therefore, a dataset that consists only of images of the head area was used. Images of all three perspectives were appended horizontally such that each sample contains the information from all available viewpoints. This is especially important as rust affected spots are sometimes only visible from some angles. The depiction below shows one sample of rust affected asparagus heads. \\
\\
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{Figures/chapter04/fe_head}
	\decoRule
	\caption[??]{??}
	\label{fig:FeatureEngineeringHead}
\end{figure}

A simple feedforward convolutional network was trained on the images. The features “flowering head” and “rust affected head” were chosen as target categories. The network comprises the input layer, three convolutional layers with kernel size two, a fully connected layer with 128 neurons as well as the output layer. For the final layer the sigmoid activation function was applied while the hidden layers have RELU activations. A dropout layer was added to avoid overfitting. The network was trained using mean squared error (MSE) as an error function. The development of loss in the learning curve indicated convergence after 40 epochs. \\
\\
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{Figures/chapter04/fe_head_curve}
	\decoRule
	\caption[??]{??}
	\label{fig:FeatureEngineeringHeadCurve}
\end{figure}

The results for both features showed to be highly specific. In contrast the sensitivity is rather low. Only 55\% of the asparagus pieces labelled as “flowering head” were identified as such whereas the true positive rate is only 19\% for “rust head”. Given the low labeling agreement for these criteria these mediocre results are not surprising. \\
\\
The ROC curve indicates how the classifiers respond to the introduction of a bias and shows the overall prediction quality. The area under the curve is small for the feature “rost head”. Beside incongruencies in the labels this is possibly due the choice of the head region. It might be the case that brown spots in regions other than the cropped head were considered as an indicator for a rusty head when attributing labels. Improvements by increasing the cropped head region appear to be possible. \\

\begin{tabular}{lrrrrrr}\n\\toprule\n{} &  False positive &  False negative &  True positive &  True negative &  Sensitivity &  Specificity \\\\\n\\midrule\nhas\\blume     &            0.04 &            0.06 &           0.08 &           0.82 &         0.55 &         0.95 \\\\\nhas\\rost\\head &            0.02 &            0.13 &           0.03 &           0.83 &         0.19 &         0.98 \\\\\n
\bottomrule\n
\end{tabular}

\\
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{Figures/chapter04/fe_head_roc}
	\decoRule
	\caption[??]{??}
	\label{fig:FeatureEngineeringHeadROC}
\end{figure}


\subsubsection{Single-label classification}

One supervised approach often used for training a model on image data is a convolutional neural network (CNN)~\citep{geron2019hands} ~\citep{lecun1995convolutional}. The approach was tested for our data with the application of a CNN using single-label classification on features. CNN apply trainable filtering kernels to extract features from an input image and project these features onto feature maps ~\citep{geron2019hands} ~\citep{bishop2006pattern} ~\citep{lecun1995convolutional}. Based on these maps, the label is predicted by a standard fully-connected layer. \\
Though using a CNN on the raw data images seemed promising, the results were only slightly better than chance level for the training accuracy. \\
\\
When checking the asparagus images, an idea was to test whether the raw images with background and reduced to every 6th pixel (as created in chapter 3.5) can be trained on a simple CNN using single-label classification. That is, the network tries to label the images for one of the 13 features that were manually extracted for the training image triplets. The data the model was trained on is stored in the grid folder XX and the labels were retrieved from the new\textunderscore combined.csv at XX. The script to the model can be found on Github under XX. \\
As a starting point, a general model structure was needed as inspiration for a CNN. For example, the visual geometry group networks (VGGs) with varying depth seemed to be a good choice for image classification as their VGG16 had won the ImageNet challenge of 2014 and is often implemented for image classification tasks (\url{https://neurohive.io/en/popular-networks/vgg16/}) (~\citep{vgg2014original}). However, there are two major drawbacks on using them, namely their depth and the amount of fully-connected nodes which makes them slow to train and in need of a lot of memory space (\url{https://neurohive.io/en/popular-networks/vgg16/}) ~\citep{zhang2015accelerating}. Part of these problems also arise with even deeper networks like ResNet ~\citep{resnet2016original} (\url={https://neurohive.io/en/popular-networks/resnet/}). Thus, AlexNet was chosen as a blueprint for the CNN because it is a small network in relation to other nets while still performing comparatively good (\url{https://neurohive.io/en/popular-networks/alexnet-imagenet-classification-with-deep-convolutional-neural-networks/}) ~\citep{alexnet2012original} ~\citep{geron2019hands}. As the variance in the data images is relatively small it was assumed that not as many layers are needed as employed in deeper networks like VGG~\citep{geron2019hands}. \\
\\
In general, the model architecture is roughly based on AlexNet but it was strongly simplified to the level of variability and complexity of the underlying data. The network comprises four hidden layers: a convolutional layer, followed by a pooling layer, a second convolutional layer, and a dense layer. The input is an array of multiple horizontally stacked images. This input is trained on a set of binary labels containing information on whether the respective feature applies to the current image. The output of the network gives a prediction on each entered image gated by a sigmoid function on a range between 0 and 1. The rounded integer values of this output give a prediction of the apparent label.
For the training phase of the model, the Adam optimizer was used because of its general acceptance as the state of the art optimizer for backpropagation ~\citep{kingma2014adam} (\url{https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c}). As a loss function, binary cross entropy was used as it promised good results for binary single-label classification tasks~\citep{geron2019hands} (\url{https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a}) (\url{https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6}). \\
When training artificial neural networks, it can be difficult to find clear guidelines on how to implement an architecture such that an optimal training performance is given ~\citep{heaton2015aifh} ~\citep{geron2019hands} (\url={https://medium.com/tebs-lab/how-to-classify-mnist-digits-with-different-neural-network-architectures-39c75a0f03e3}). Hence, the idea was to start with the simplest form of a CNN and then gradually increase the complexity of the network. While AlexNet provides a good baseline for an image classification network, its architecture was still assumed to be unnecessarily complex for the given task. First, the architecture was reduced to the minimum number of layers and parameters needed for a CNN. Over the period of training optimization, various processing steps and hyperparameters were implemented and compared according to their performance. During this process, the data was split between 12000 samples for training data and 1319 validation data in order to have a reasonable overview on the possible test performance and to prevent direct overfitting. The data used as test data was randomly chosen from the whole data set. \\
The course of the hyperparameters is explained in the following.
The batch size was initiated comparatively low with 64 samples per batch but soon increased to a value of 512 samples. The larger batch size was implemented in order to guarantee the convergence of the training data, since smaller batch sizes resulted in jumping gradients, which were not able to converge into any minimum~\citep{bengio2012practical} (QUELLE). \\
As for the learning rate, various learning rates were tested during the optimization process. The initial learning rate of 0.003 (which is the standard learning rate for Adam optimizers in Tensorflow~\citep{kingma2014adam} ~\citep{geron2019hands}) was soon found to be too large to guarantee convergence of the algorithm. Thus, the learning rate was gradually decreased and found to be most effective in the range of 0.000001 (1e-6) to 0.00000001 (1e-8). Learning rates as small as 0.00001 (1e-5) still seemed too large for training the net. In addition, a gradually decreasing learning rate was implemented in order to make the training more effective~\citep{bengio2012practical}, even though, in theory, the Adam optimizer already manages learning rate decay~\citep{kingma2014adam}.
Starting with the smallest layer size possible, more layers were added. For example, for the feature ‘fractured’ one layer for the edge detection should have sufficed, or a layer for colour detection for the feature ‘violet’. For other, higher-level features, such as bent, more convolutional layers were expected to be more helpful~\citep{geron2019hands}. During the training process, it was settled to a model with two convolutional layers, one max pooling layer and one hidden dense layer. \\
A small number of kernels was thought to be enough compared to the number of kernels of AlexNet since for single-label classification fewer kernels were expected to be needed (QUELLE). The kernel size was picked to be 32 5$3\times4$ 5 kernels for the first convolutional layer and 64 3$3\times4$ 3 kernels for the second convolutional layer. These sizes were assumed to be sufficient, especially since increasing the number of kernels did not lead to any better results. \\
Both convolutional layers were built with batch normalization nearly from the start of implementation of the network. To guarantee that exploding or vanishing gradients pose no further problem, the gradients were inspected visually and the results gave no reason for assuming problems with exploding or vanishing gradients~\citep{pascanu2012understanding}.
A next step was to weight the loss function because of the largely unbalanced data~\citep{he2009learning} ~\citep{batista2004study}. This ended with no better results, however, as the model still tended to make an unbalanced prediction by classifying all values as negative samples. Another idea was to reduce the dataset to make the number of images with the regarding feature present even to the number of images where it is absent. This translates to throwing away valuable data, which can otherwise provide information about negative cases to support the model in it’s training~\citep{batista2004study}. Thus, it was decided to keep all data images and instead balance the data by multiplying the minority of samples to match the number of contrary samples. As there was no feature positively exceeding a presence of 50\% in the data, solely positive labels were oversampled. The balancing was only performed on the training data, while the test data was not changed. \\
To improve training performance, some kinds of data augmentation were implemented. First, the images were flipped horizontally. This seemed to be a valid enhancement to the training data since it resulted in similar images to the original data (\url{https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/}). Additionally, small changes in image angles (up to 5°) were tested, however, this type of data augmentation was neither effective nor convenient to be computed. \\
In most cases, around 300 training steps were performed for the training, translating to XX epochs. \\
\\
The results of the CNN showed for all features, that the training accuracy as well as the test accuracy converges. Further, the model does not overfit. The training loss and the test loss both decrease, with the training loss usually decreasing from XX to XX, and the test loss from around XX to XX. However, the training accuracy is not notably better than a ‘best guess prediction’ (always predicting one class). In no case the train accuracy considerably exceeded 50\%. The test accuracy usually increased up to XX - XX. Sensitivity and specificity were, put together, often not considerably higher than 1. \\
Best results were achieved for the feature XX with a sensitivity of XX and a specificity of XX, while worst results were achieved for XX with a sensitivity of XX and a specificity of XX. \\
The results can be interpreted in that the simple CNN seems not able to notably learn better than a “best guess prediction”(= predicting only 0.0 or 1.0).
For the data of this format, the model converges fast to a minimum. However, this minimum seems to be the “best guess” minimum and it is questionable whether it is possible to find the global minimum on the error surface by using a gradient descent algorithm. It might be that the network is not deep enough to find the right parameters to separate positive from negative samples (QUELLE). However, a further increase of hidden layers by adding a third convolutional layer and a second pooling layer did not lead to better results than for the original model. Added together, sensitivity and specificity were never considerably above 1 but usually below it. Either the network was good at predicting the feature to be present (1.0) or the feature not to be present (0.0) which essentially translates to the problem of not finding a minimum better than the “best guess prediction” (QUELLE). \\
The training accuracy converging to 50\% gave rise to questions. Even while the batch size was at 512 samples per batch and the training data was balanced, with its actual parameters the model seemed to find no hint how to distinguish the samples. \\
A further influence on the training of the model could be the unbalanced data set. It was evened out, however, by this a lot of samples were represented multiple times in the training data. This was especially the case for features with low original representation, such as e.g. the feature ‘fractured’ or the feature ‘violet’. Thus, it was expected that the model might overfit on the training data due to the oversampling of the positive samples in the training data which did not occur~\citep{he2009learning}. \\
\\
On a more general level concerning the input, there might have been trouble for the model with the conformity of the data. As the samples were labelled by humans, there is a subjective bias in the data that might stop the model from finding a global difference between positive and negative samples (QUELLE). \\
Another factor poses the use of image triplets. It might have been easier for the network to learn to distinguish the data samples if single images would have been manually labelled instead of image triplets (QUELLE). \\
One idea for optimizing the process and to reach better results for features like, e.g. rust or violet which rely on a colour parameter, could be to simplify the input for the network (QUELLE). That is, the images are prepared in a way to guide the network into the direction of parameters that we want it to discern. \\
\\
In conclusion, the simple CNN seems not able to learn from the data. The train loss decreases and converges but the train accuracy cannot considerably exceed above chance level of 50\%. Although the test accuracy is increasing this renders no further information about the classification ability of the net. Put together, sensitivity and specificity rarely reach above 1. Rather, one of both tends to reach close to 1 while the other is close to 0. Further preprocessing of the image data before training might have improved the results.



\subsubsection{Multi-label classification}


Building on the standard single-label classification we were further interested whether it would be possible to build a model that can predict several feature labels at the same time. A multi-label classification model hereby gets an image as the input and learns to predict the presence or absence of  the feature labels. \\
For our model we decided to use a small convolutional neural network as described below and the features that we labelled by hand. Each of the six features (hollow, flower, rust head, rust body, bent and violet) is encoded by a binary output in the target vector, indicating whether the asparagus exhibits the feature in question or not. \\
\\
BACKGROUND \\
Multi-label classification is a useful tool for classification problems in which several classes are assigned to a single input. In contrast to a multi-class classification, where the model is supposed to predict the most likely class for an input, the multi-label classification makes a prediction for each class separately, determining whether the class is present in the image or not. While the different classes are mutually-exclusive in the multi-class classification, they can be related in the multi-label classification. Further, there is no limit on how many classes can be depicted in one image. It is possible that all or none of the classes are present. \\
One could think of the multi-label classification task as consisting of different sub-tasks. Therefore, some multi-label classification problems can be transformed. There are two eminent ways to do so, either the problems  are transformed into multiple binary classification tasks or into one multi-class classification task. \\
In the first approach, a new model for each feature is trained, which are then combined to give a single output. That means that all features are independent of one another, because they are learned separately. This can be seen as one of the major drawbacks as it is not always clear whether features are related or not and in many cases they are.
Therefore, we decided to not only use single-class classification as described in chapter 4.1.1 but to explore the possibilities of multi-label classification. \\
In the second approach, each possible combination of features is interpreted as one class. For a classification problem with 6 features that means there are 64 classes to be learned. The problems with this approach are, on the one hand, the exponentially increasing number of classes, and on the other hand the sparsity of samples per class. In many cases, some of the classes will be highly underrepresented or even empty. For that reason we decided not to elaborate this approach further and implement a model for multi-label classification without transforming the task to a multi-class problem. \\
\\
Inspiration for the model gave a blogpost (\url{https://towardsdatascience.com/multi-label-classification-and-class-activation-map-on-fashion-mnist-1454f09f5925}) which aims to classify images of the MNIST fashion dataset in the context of multi-label, rather than multi-class classification. The author altered the dataset in such a way that each input image contains four randomly selected items from the MNIST fashion dataset. The model then learns to predict which classes are present in the image. There is no restriction on the random selection, therefore the items can be all from the same class, from four different classes or anything in between. The target vector has 10 values, one for each class, which are either 0 or 1 depending on whether that class can be found in the input image or not. \\
\\
This model was chosen as inspiration for two main reasons. Firstly, it tackles a similar problem as ours. In both cases, the model is supposed to predict the presence or absence of different fashion items or labelled features, respectively, and the number of classes is similar, it is 10 in the fashion example and 6 for our model. \\
Secondly, the model uses a dataset of similar size, 9000 images were used for training and 1000 for validation, while we were training on 10800 images and validating on 1200. Despite the rather small dataset in comparison to many other machine learning problems, good results with an accuracy of 95-96\% were reached (QUELLE \url{https://towardsdatascience.com/multi-label-classification-and-class-activation-map-on-fashion-mnist-1454f09f5925}). This leads us to think, it might be a model with a good complexity for our problem too, as it is complex enough to model the underlying distribution, but not too complex for the medium-sized dataset.
 \\
\\
MODEL STRUCTURE \\
A classical convolutional neural network was chosen for the multi-label classification task. It consists of five blocks of convolution layers with max pooling layers followed by a global average pooling layer and a dense layer. \\
\\
In contrast to multi-class classification models, where usually a softmax activation function is used in the last layer together with a categorical cross entropy loss, the multi-label classification model uses a sigmoid activation function and a binary cross entropy loss. \\
\\
As the input of the model a concatenation of the three perspectives of each asparagus piece were used in order to maximize the information the model gets about each asparagus. This yields input images that look like the three asparagus pieces are laying side by side. Further the images were downscaled by a factor of 6 to facilitate training. \\
\\
The output, or target, of the model is a vector of length six in which each position encodes one of the six hand-labelled features (hollow, flower, rust head, rust body, bent and violet). Each feature can either be applicable to the input or not, which leads to a "1" or "0" in the target vector, respectively. \\
\\
Several loss functions were tested to improve the models performance. For example, the hamming loss, which uses the fraction of the wrong labels to the total number of labels. Additionally to the in-built loss functions from keras, a custom loss function was implemented, that penalizes falsely classifying a feature as present more than falsely classifying one as absent. The motivation for this custom loss was the fact that the two labels "0" and "1" are highly unbalanced. As previously stated, there are noticeably more zeros than ones in many classes. To be more precise, the model can get an accuracy of 77\% by labeling all features as zero. By penalizing this error more, we intended to counteract the unbalanced dataset. But at the end, the binary cross entropy loss remained the one with the best results. \\
Further, it was tested whether regularization would improve the performance of the model on the validation data by preventing overfitting. For this, the model was trained by adding L1 or L2 regularization, respectively, to all five of the convolutional layers.  Hereby, a kernel regularization was implemented with a value of 0.01. \\
\\
L1 and L2 regularization can both be interpreted as constraints to the optimization that have to be considered when minimizing the loss term. The main difference between the two is that L1 regularization reduces the coefficient of irrelevant features to zero, which means they are removed completely. Hence, L1 regularization allows for sparse models and can be seen as a selection mechanism for features. As the inputs to our model are images, that consist of a large number of pixels, and additionally a large portion of those pixels are black, because the background was removed, it appears to be a good idea to reduce the number of features taken into account in the early layers. L2 regularization, on the contrary, does not set coefficients to zero, but punishes large coefficients more than smaller ones. This way the error is better distributed over the whole vector. \\
\\
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{Figures/chapter04/multilabel_structure}
	\decoRule
	\caption[Model Structure]{This table shows the structure of the multilabel classification model. It describes which layers were implemented, how the output changes in each layer and how many parameters were trained in each layer and in total.}
	\label{fig:MultilabelStructure}
\end{figure}

RESULTS \\
As one can see in the figures XX-YY, all the different approaches explained above show a similar behaviour in accuracy and loss values. The training and validation accuracy increase slowly but steadily with the training accuracy always being a little higher than the validation accuracy. The training loss decreases rapidly, while the validation loss only decreases very little and shows random fluctuations. This can be an indicator for overfitting. Usually, L1 and L2 regularization are used to prevent overfitting, but in our case it did not improve the results, as one can see in figure XX. \\
When looking at the true positive rates, also called sensitivity, and the true negative rates or specificity, one can see that both increase during the training process , while the false negative and false positive rates decrease with the same slope. The false positive and false negative rates are mirror images to the true positive and true negative rates with the mirroring axis at the 50\% mark. It can be observed that the rates change rapidly in the first two to four epochs, after which the change progresses slowly in the same direction with no greater disturbances. One exception is the model trained with the L2 loss, which does not show a large change in either of the rates. \\
\\
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.8]{Figures/chapter04/multi_crossent}
	\decoRule
	\caption[Binary Cross-Entropy Loss]{These graphs show the evaluation of the training with binary cross-entropy loss. The model was trained over 50 epochs and the accuracy and loss was measured. Further, the false/true positive/negative rates were determined.}
	\label{fig:MultilabelCrossentropy}
\end{figure}
\\
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.8]{Figures/chapter04/multi_ham}
	\decoRule
	\caption[Hamming Loss]{These graphs show the evaluation of the training with hamming loss. The model was trained over 50 epochs and the accuracy and loss was measured. Further, the false/true positive/negative rates were determined.}
	\label{fig:MultilabelHammingLoss}
\end{figure}
\\
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.8]{Figures/chapter04/multi_costum}
	\decoRule
	\caption[Costum Loss]{These graphs show the evaluation of the training with costum loss that punishes falsely classified ones more than falsely classified zeros. The model was trained over 50 epochs and the accuracy and loss was measured. Further, the false/true positive/negative rates were determined.}
	\label{fig:MultilabelCostumLoss}
\end{figure}
\\
When comparing the three different loss functions, it is noticeable that the binary cross entropy loss has significantly larger accuracy values than the hamming loss and the costum loss. Its values start at 75\%, while they start at around 30\% for the other two loss functions. The behaviour of the curves and the (mis-)classification rates, however, are very similar in all three approaches. The specificities start off very high with values around 78,46\% for the binary cross entropy loss, 78,73\% for the hamming loss and 74,74\% for the costum loss, and increase further during the training. The highest values are reached with the binary cross entropy loss (91,41\%) , closely followed by the hamming loss (91,37\%) and the costum loss (88,75\%). The sensitivity values start off lower, at around 37\% to 42\%, and increase rapidly in the first few epochs, after which the rates proceed to increase but with a narrower slope. They reach values of up to 67,27\% with the binary cross entropy loss, 68,17\% with the hamming loss and 68,17\% with the costum loss. As stated above, the false negative and false positive rates show the same slope but in the opposite direction. \\
\\
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.8]{Figures/chapter04/multi_reg1}
	\decoRule
	\caption[L1 Regularization]{These graphs show the evaluation of the training with L1 regularization. As the loss function the binary cross-entropy loss was used. The model was trained over 25 epochs and the accuracy and loss was measured. Further, the false/true positive/negative rates were determined.}
	\label{fig:MultilabelL1Regularization}
\end{figure}
\\
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.8]{Figures/chapter04/multi_reg2}
	\decoRule
	\caption[L2 Regularization]{These graphs show the evaluation of the training with L2 regularization. As the loss function the binary cross-entropy loss was used. The model was trained over 25 epochs and the accuracy and loss was measured. Further, the false/true positive/negative rates were determined.}
	\label{fig:MultilabelL2Regularization}
\end{figure}
\\
\\
The accuracy values of the models that were trained with L1 or L2 regularization, respectively, do not change over the epochs. The same holds for the validation loss. The training loss decreases in the first few epochs and remains stable after that.
While the (mis)-classification rates of the model trained with L1 regularization behave similarly to the ones trained with no regularization, the rates of the model trained with L2 regularization show a smaller increase and lack the fast change in the first epochs. \\
\\
DISCUSSION \\
The slopes of all curves indicate that the model is learning, because they are increasing in the case of the accuracy, sensitivity and specificity and decreasing in the case of the loss, false positive and false negative rates until the end of training. Hence, one might think that a longer training period will lead to better results. But the training loss decreases very rapidly while the validation loss does not. This suggests overfitting of the model, a problem which gets worse when increasing the training steps. Therefore, a longer training period most likely will not increase performance unless overfitting is prevented. As one can see in the result section neither L1 nor L2 regularization alone were able to prevent overfitting. Another common practice that could be tested is the drop-out, in which a certain amount of nodes are left out in different backpropagation steps. This way the model learns to not rely on a small number of nodes but distribute the information between all nodes available. Hence, the coefficients remain smaller. Another way to prevent overfitting is to reduce the model's complexity. A model with fewer parameters to train, is less prone to overfitting. A fitting degree of complexity should be found to model the data sufficiently good without losing the possibility of generalization. \\
\\
Accuracy alone might not be a good indicator to evaluate a multi-label model~\citep{gibaja2015}. As it highly depends on the loss function, it may have misleading results. This can be seen in the comparison between the three different loss functions. Although the sensitivity and specificity show similar values, the accuracy values suggest that the binary cross entropy loss outperforms the other two loss functions by far. The accuracy of the model trained with the binary cross entropy loss has an accuracy more than twice as high, but when looking at the slope of the curve it appears that the model with the binary cross entropy loss does not, in fact, perform better than the other two models, because all three have an increase of accuracy of roughly 10\% and a similar sensitivity and specificity. This indicates that the slope of the accuracy function can be considered to evaluate the training process of the model, but the real values should be interpreted with caution. \\
\\
One thing that comes to attention when looking at the (mis-)classification rates is that the sensitivities are a lot lower than the specificities. A reason for that might be that there are fewer positive values in the dataset and they are, thereby, more difficult to learn. The model might have learned that, if unsure, a zero is the more likely guess. \\
\\
The L1 and L2 regularization both seem to prevent the model from learning all together instead of only preventing overfitting. A reason for that might be that the regularization factor was too high. More experiments should be conducted with varying values to test this hypothesis. \\
\\
In summary, the model improves its sensitivity and specificity, but it seems like it does so by overfitting the training data. Therefore, the next step should be to prevent the model from overfitting and after this problem is solved, it should be tested whether additional changes can improve the performance of the model further.



\subsubsection{From feature to label}

Approximately 200 asparagus pieces per label class were pre-sorted [cf. Chapter 1.4] and served as ground truth mappings between input images and output class labels. These images were manually annotated with features (see Chapter X.X and Table X.X table with the features). This allowed us to break up the classification process into two steps: In the first step we predict feature values from images and in a second step we predict class labels from feature values. \\
\\
This chapter deals with the second step: using supervised learning to predict 13 ground truth class labels based on manually labelled features. We build a unified interface to load different models, train them and analyze their predictions. It provides compatibility for scikit-learn as well as for keras models. To explore the data and visualize the results, a streamlit app was built(Footnote: the source code can be found in the GitHub repository at code/pipeline). This has the advantage that the user can easily load a model which is automatically trained, select an asparagus piece, see the corresponding pictures and the predicted class label. The user is sees the distribution of the selected data (Figure XX) and can inspect the absolute and relative number of correctly and incorrectly classified asparagus pieces in the confusion matrix (Figure XX). n a confusion matrix (Figure X)", "overall distribution of true positives and negatives (Figure Y)" etc. We can clearly see that … Furthermore, the user can see the images of an individual asparagus piece and check if  his/her expectations for the manually labelled features are satisfied and if the class label of the piece is predicted correctly (Figure XX). \\
\\
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.8]{Figures/chapter04/feature_to_label}
	\decoRule
	\caption[??]{??}
	\label{fig:FeatureToLabel}
\end{figure}


Two exemplary models were implemented and tested to predict the classes from the features. The first one is a random forest~\citep{breiman2001random}, with 100 trees. \\
\\
Random forests are a popular machine learning approach, because they reach good results in both regression and classification tasks. Further, they are based on decision trees which offer an intuitive interpretation. Although decision trees themselves are powerful tools, they are prone to overfitting. Random forests aim to avoid this problem, while still using the advantages of decision trees, namely that they are flexible, easy to use and fast to train. They do so by training several decision trees on different random parts of the data, after which a majority voting decides on the final output or class. An additional trick is to use a random selection of features at each branch. This reduces the influence of highly correlated features and thereby makes the random forest more robust. \\
\\
The second model to predict classes from features is a multilayer perceptron (six fully connected layers). It was only implemented to show how to integrate other models, but achieves a similar score as the random forest classifier when trained for XXX epochs (score of 0.76). However, it takes longer to train as the random forest classifier. \\
\\
The score of 0.76 on the VALIDATION SET means that  … . F1 / Precision / Recall Compared to the accuracy that would be achieved by random guessing (0.08 for 13 uniformly distributed classes), the random forest model can classify 76\% of the asparagus pieces in the validation set correctly. \\
There are still many unanswered questions about how the selection of features or the agreement on the values of the labelled features influence the performance of the described classifiers. Further work is required to implement the decision tree described by the local farmer [cf. Chapter 1.3]. One could test if this decision tree which is based on expert knowledge can outperform the random forest trained on the training samples. 



\subsection{Unsupervised learning}

Unsupervised learning are all kinds of machine learning algorithms which are not supervised. More specifically, they work without a known goal, a reward system or prior training, and find a structure within the data, or some form of clustering of data points. In supervised approaches, the model is given both the input and the labels. In unsupervised learning approaches, the model only receives the input data. Thus, unsupervised learning works without training samples, and without labelled data. The goal of unsupervised learning algorithms is to find hidden structures in the data.  \\
\\
Dimension reduction algorithms and clustering algorithms have been identified as the two main classes of unsupervised machine learning algorithms which are used in image categorization~\citep{olaode2014}. \\
\\
Multivariate datasets are generally high dimensional. However, it is common that some parts of that variable space are more filled with data points than others. A big amount of the high dimensional variable space is not used. In order to recognize a structure or pattern in the data, it is necessary to reduce the number of dimensions. For this, both linear- as well as non-linear approaches can be applied. Linear unsupervised learning methods for which also descriptive statistics can be acquired are e.g. Principal Component Analysis (PCA), Non-negative matrix factorization, and Independent component analysis~\citep{olaode2014}. Some examples for non-linear approaches would be Kernel PCA,  ( Scholkopf  et  al. – im oben genannten paper),  Isometric  Feature  Mapping  (ISOMAP),  Local  Linear  Embedding,  and Local  Multi-Dimensional  Scaling  (Local  MDS). \\
For the current work, the linear dimension reduction algorithm PCA was chosen.


\subsubsection{Principal Component Analysis}

PCA was chosen, because it is one of the standard unsupervised machine learning methods. Moreover, it is a linear, non-parametric method and widespread application to extract relevant information from high dimensional datasets. The hope is to reduce the complexity of the data, by only a minor loss of information~\citep{shlens2009}. Besides being a dimension reduction algorithm, PCA can also be useful to visualize the data, filter noise, extract features, or compress the data. \\
\\
Our initial aim was to reduce the dimension of our dataset for further models. As PCA was applied at the beginning of the data inspection, we also had the aim to visualize our data in a three-dimensional space, in order to get a better understanding of the data distribution. The images that comprise our original dataset have a high quality, and instead of only reducing the pixel size, we aimed for reducing the information contained in named depictions by analyzing the principal components in a first step and projecting all relevant images into the lower dimensional space. This information could serve as the input for later machine learning algorithms. As we decided to use an approach for semi supervised learning that is based on neural networks later we focussed on the question what principal components reveal about the data. \\
 \\
PCA relies on linear algebra. A general assumption especially with respect to images is that large variances are accompanied by important structure. The covariance matrix of the data reveals information about the overall structure and orientation of the data points in the multivariate space. The axis with the largest variance is set as the first principal component. An additional assumption is that the principal components are orthogonal to each other. Therefore, the second principal component is the highest variability of all directions which are orthogonal to the first one ~\citep{Bohling2006}. The covariance between each pair of principal components is zero, as they are uncorrelated and generally, the higher the eigenvalues, the more useful it is for the analysis. As eigenvalues specify the variance of the data, more eigenvalues indicate more variation about more features. \\
\\
The result of a PCA is a representation of the data in a new lower coordinate system, which depends on the axes of the largest variance. The dimensionality of this lower coordinate system should depend on the values of the eigenvalues. When plotting the data along the axes of the principal components, it is often easier to understand and interpret the data, than in the original variable space.  \\
\\
It is a consensus in the literature, that PCA can be a good method to apply dimension reduction on images~\citep{turk1991face}~\citep{lata2009}.
However, there are several different ways on how to do so. First of all, it has to be decided if the PCA should be performed on a black and white image, or if the structure of the data requires the use of colorized images. When working on black-and-white images, only one data point per pixel is given, therefore, performing a PCA is less computationally expensive, and also finding structure is easier. However, as we need to be able to recognize violet as well as rust, it was important to us, to be able to differentiate between color nuances, which cannot be represented in a black and white image.   \\
 \\
There are three different opportunities regarding our project which can be considered as useful ways on on how to perform a PCA. First, it can be performed on images of different classes at the same time – similar as to capture several images of several people in one database, on which a PCA is performed. In our case this would mean that a PCA is applied to a dataset of several input images of all 13 classes of asparagus images. Second way would be to perform a PCA separately on each group. This way, an “Eigenasparagus” in each group would be calculated, and distances between the “Eigenasparagus” of different groups could be measured. Third PCA can be employed feature-wise. In this case, the dataset would consist of a collection of images with a certain feature present vs a collection of the same size with the feature absent.  \\
 \\
We calculated the PCA for black-and-white images, as well as on images without background, and also tried to work on all groups at the same time, group-wise as well as feature-wise. We decided to perform our final PCA on sliced RGB images with background, that are labelled with features by us with the hand-label-app, as this seems to yield the best results. An amount of 400 pictures per feature was considered to later on perform a binary PCA for each feature (either the feature is absent or present). \\
\\
The 200 pictures where the certain feature is present as well as the 200 pictures where the certain feature is absent are extracted through a function in code/pca\textunderscore code/feature\textunderscore ids.py. This function loops over the combinded\textunderscore new.csv csv-file, where all hand-labelled information is stored as well as the path to the labeled pictures. For each feature a matrix is created, storing 200 pictures with the present feature and 200 pictures without the feature. E.g., m\textunderscore hollow is the matrix created for the feature hollow (shape = 400, img\textunderscore shape[0]$\times$ img\textunderscore shape[1]$\times$ img\textunderscore shape[2]). The first 200 entries in the matrix are pictures of hollow asparagus, the last 200 pictures show asparagus, which is not hollow. These matrices were calculated for the features hollow,  flower, rusty head, rusty body, bent, violet, length and width. The data points of those 400 images in 2D space can be seen in figure XX (die figure mit den scatterplots).  \\
\\
For all these features a PCA is calculated by first standardizing the matrix pixelwise (dimensions: 1340$3\times4$ 346$3\times4$ 3), calculating the covariance matrix and then extracting the ordered eigenvalues. The principal components are calculated multiplying these eigenvectors with the standardized matrix. The highest 10 eigenvalues were plotted to visually decide where to set the threshold of how many principal components will be further included (see figure XX - die figure mit dem graph - die ersten 10 eigenvalues). The feature space, the principal components and the standardized matrices are saved to later perform a recognition function. The first ten “Eigenasparagus” for each feature can be seen in figure XX (die schön bunten spargelbilder).  \\
\\
The recognize function is a control function, which performs on unseen images and tries to predict if a feature is absent or present. This function is also performed feature-wise. It reads in a new picture of one asparagus, which is not part of the asparagus database, so not within the 400 images. Then, it searches for this picture the most similar asparagus in the feature matrices (which are 200 pictures with the feature, 200 pictures without).  \\
In greater detail, the input picture is first centered by subtracting the mean asparagus and then the picture is projected into the corresponding feature-space. That means the picture is “translated” into the lower dimensional space, in order to compare it to the known 400 pictures. The comparison is made through calculating the distance between the single centered eigenasparagus and the 400 pictures in the feature space, by using the cdist function of the SciPy software. The smallest distance is considered as the most similar asparagus. If the index of the most similar asparagus is smaller than 200 we know that the feature is present, if it is above 200 the feature is absent. By comparing this to the information of the single asparagus piece, we know if the new asparagus has the same feature as its closest asparagus in the feature space, or not. By doing this for several images, we can already presume if the two features are likely to be easily separable or not. By evaluating this, we have a measure of how well our used principal components capture the distinguishing information of each feature.\\


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.09]{Figures/chapter04/pc_bended}
	\decoRule
	\caption[??]{??}
	\label{fig:PrincipalComponentBended}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.09]{Figures/chapter04/pc_blume}
	\decoRule
	\caption[??]{??}
	\label{fig:PrincipalComponentBlume}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.09]{Figures/chapter04/pc_hollow}
	\decoRule
	\caption[??]{??}
	\label{fig:PrincipalComponentHollow}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.09]{Figures/chapter04/pc_length}
	\decoRule
	\caption[??]{??}
	\label{fig:PrincipalComponentLength}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.09]{Figures/chapter04/pc_rust}
	\decoRule
	\caption[??]{??}
	\label{fig:PrincipalComponentRust}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.09]{Figures/chapter04/pc_violet}
	\decoRule
	\caption[??]{??}
	\label{fig:PrincipalComponentViolet}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.09]{Figures/chapter04/pc_width}
	\decoRule
	\caption[??]{??}
	\label{fig:PrincipalComponentWidth}
\end{figure}


The final results of the PCA are given feature-wise and were not translated back on the initial 13 groups. The features on which results are given are: hollow, flower, violet, rusty body, bent, width and length. All features are binary. For the first five features, the manually hand-labelled information was taken, for the last two features, the information, which was extracted by the algorithms used in the hand-label app were taken. The decision boundary for the first five therefore depends on our predefined criteria on labeling. The decision boundary for width was narrower or equal to 20 mm or wider than 20 mm, for length shorter or equal to 210mm or longer than 210 mm.  \\
\\
There are no results for the feature broken, even though it is one of the initial main features, as there were only five labelled pictures for this category. Therefore, it was excluded for further analysis. For the feature rust head, a calculation problem emerged during the process. For an unexplainable reason, there were complex values in the calculation of the principal components. Due to time constraints, this problem was not solved, yet. Therefore, plotting of the feature rust head was not possible.  \\
\\
By applying the feature\textunderscore pca.py file on each feature, the eigenvectors, eigenvalues, and principal components for each feature were computed and stored. In all cases, the first principal component is quite high (between 286,94 and 254,78), and drops down rapidly afterwards. The values of the principal components after the fourth principal component are very low (<= 9).  \\
\\
After inspection of the graph of the first 10 principal components,  only the  first four principal components were used for the analysis, as there was either a sharp drop in the slope after the first 4 or to maintain continuity between the different features. Following, the corresponding feature space is that space spanned by the first four principal components.  \\
 \\
We plotted the four-dimensional data in the feature space as scatterplots in three-dimensional space and the fourth dimension as color, but as it was difficult to interpret and visually understand, we decided to show plots of only the first two dimensions, here. \\ 
\\ 
The scatterplots in Figure~\ref{fig:PrincipalComponentScatter} show the data of each feature lined up along the axes of the first two principal components of each feature. \\

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.8]{Figures/chapter04/pc_2dspace}
	\decoRule
	\caption[Featurewise scatterplot in 2D]{Plotting the data in the 2D PC space for each feature.}
	\label{fig:PrincipalComponentScatter}
\end{figure}
\\
For the recognition function, we used the same 10 images, to test each feature. The classification worked best for the features length and hollow (10/10 classified correctly) and then width (8/10 classified correctly). It performed around chance-level for flower (6/10 classified correctly), violet and rusty body (5/10 classified correctly) and extremely poor for bent (2/10 classified correctly).   \\
\\
 
DISCUSSION \\
The results we got for the final PCA approach, of calculating the principal components for each feature separately led to better results, then the first two approaches.  \\
 \\
From the images of the first 10 principal components, we can visually assume that there is information about the length and shape stored in the first principal component, as a clear asparagus piece can be seen. The following images leave a lot of room for interpretation, about what information is contained there.  \\
\\
We performed the PCA on each feature separately, to extract the principal components of each feature. It is interesting to see that the pictures of the different features are all very similar. One reason for this might be that many of the 400 pictures for each feature are overlapping between features. Another reason might be, that even though the images vary between features, the general information of all asparagus images is very similar. \\
\\
From the results of the recognition function, one can see that there are large differences between the features on how well our PCA performs (20\% - 100\%). 
One reason for this could be that certain features are simply more difficult to distinguish than others. Another reason for this large variation can be that certain features are also more difficult to label consistently (see also chapter 3.5.4  (agreement measures)), and that the results are due to inconsistencies within the data. One indicator that this is a considerable reason is that the performance of the width and length features, which is information which is not hand-labelled, is very high. Moreover, the poorest results can be observed for the features bent and rusty body. Those are the features, for which the agreement measures showed the largest discrepancies between annotators (see 3.5.4).  \\
 \\
Another reason why the results are partly only moderate, is that RGB image data possesses complicated structures, and by representing it in a linear low dimensional feature space, it might be that simply too much information is lost. Even though there are papers reporting good results on PCA on image data~\citep{turk1991face} ~\citep{lata2009}, there are other papers claiming that nonlinear dimension reduction algorithms are needed for image data ~\citep{olaode2014}.


\subsubsection{Autoencoder}

Beside PCA, there are further techniques for dimensionality reduction. One alternative machine learning technique that can be employed to deduce sparse representations and automatically extract features by learning from examples are autoencoders. Simple autoencoders, where the decoder and encoder consist of multilayer perceptrons, were already proposed as an alternative to PCA in early days of artificial neural networks when computational resources were still comparatively limited (Kramer 1991). Today one can choose from a multitude of network architectures and designs that all have one property in common: A bottleneck layer. For image classification it is common practice to use convolutional autoencoders. There are numerous papers that employed convolutional autoencoders in various domains. Examples range from medical images to aerial radar images (Chen et al 2017). These include not only shallow networks but more recently the benefits of deep autoencoders were demonstrated (Geng et al. 2015). In addition, more complex architectures like ones that combine autoencoders with generative adversarial models were proposed recently (Bao et al. 2017). In many cases the purpose of autoencoders is dimensionality reduction and feature extraction or in other words the learned latent space of the bottleneck neurons. In other cases, autoencoders are used to map images from one domain to another, for example camera recordings to label-images such that after training a labelled image can be retrieved from the decoder’s output layer (Iglovikov 2018). In short, there are many possible ways to apply autoencoders and many architectures to realize them. \\
\\
This motivates the question of how autoencoders work. As mentioned all autoencoders have a bottleneck layer. If applied for dimensionality reduction autoencoders are usually used to predict the input, in this case the image, with the input itself. Autoencoders consist of an encoder that contains the initial layers as well as the bottleneck layer and the decoder that maps the respective latent space back to the image. The desired mapping function of the input to a sparse representation is generated as a side product of the optimization in end to end training, as weights of the decoder are trained such that meaningful features are extracted. The main difference to PCA is that nonlinear functions can be approximated. Feedforward ANNs such as the encoder are non-linear function approximators. Networks with multiple layers are especially well known for establishing named nonlinear correlation. Hence, autoencoders allow for non-linear mappings to the latent space. This means that in the latter multiple features may be represented in a two dimensional space. It shows that compared to PCA where one dimension typically corresponds to one feature more information can be represented in fewer dimensions. Different properties of the input are mapped to different areas of the latent space. In this case, a convolutional variational autoencoder was used. In variational autoencoders a special loss function is used that ensures features in latent space are mapped to a compact cluster of values. This allows for interpolation between samples by moving on a straight line because regions between points in the latent space lie within the data and hence reconstructions of the decoder are more realistic. Other than that variational autoencoders share most properties with regular autoencoders. The location of a point in latent space refers to a compressed representation of the input.These can be interpreted as features of the input. \\
\\
Different variational autoencoders were tested in the realm of the project. First, a simple variational autoencoder with a multilayer perceptron as decoder was implemented. The second approach was a comparatively shallow convolutional variational autoencoder. The third approach relates to an autoencoder with a deeper encoder that was later used to design the networks for semi supervised learning. As the third approach did not improve the mapping of the properties of asparagus pieces to a two latent asparagus space the results for the second of named networks are described here. It was derived from a standard example for convolutional variational autoencoders (Keras 2020a). The presented results are for a network that comprises two hidden convolutional layers in the encoder and two deconvolutional layers in the decoder. \\
\\
Similar to the presented way of applying PCA, batches of images that contain only one perspective were used as input to the network. The downsampled dataset was used. Images had to be padded as the implementation does not work with inputs of arbitrary shape. The deconvolutional layers of the decoder can only increase dimensionality by an integer factor: The filters that were used for the deconvolution in the given net double the tensor dimensionality. For padded images, this means an increase for the vertical dimension from 34 to 68 and finally to the desired 136 pixels was achieved in the last three layers of the network (impossible for the original height of 134 pixels). The input shape which defines the shape of the output layer as well must be divisible by two without remainder twice.  \\
The depiction below shows the results. \\

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.8]{Figures/chapter04/autoencoder}
	\decoRule
	\caption[??]{??}
	\label{fig:Autoencoder}
\end{figure}

It demonstrates that the features short thick and thin are mapped to separable clusters. As a tendency curvature correlates with a region in the lower periphery as indicated especially by the deconstruction. The other features (violet, rust and blooming) are not mapped adequately and they are not visible in the reconstruction. This shows that only some features of interest were mapped to the latent space and used to decode images. Reconstructions of autoencoders are known to miss many details. Better results could potentially have been achieved using larger input images. The possibility to generate, for example, more or less curved asparagus pieces may help to define a clear cut decision boundary and classify images accordingly. As a potential feature for asparagus sorting machines this would allow the user to customize the definition of curvature to his or her own taste. For this approach to be viable for all features, however, the network performance appears to be insufficient. Some features are poorly separated by our network that was employed on downscaled images.



\subsection{Semi-supervised learning}

We collected more than 100.000 samples. Considering the uniform appearance of the images this represents a substantial amount. However, the target categories and information regarding the features present for each peace are unavailable. Labels had to be manually generated which was done for around 10.000 samples. As a consequence, there is only a small subset of data with attributed labels. Smaller amounts of labelled data mean that predictions can be successful only if the variance in the source values is limited. Hence, for high dimensional data such as images, sparse representations are desirable. Extracting features automatically instead of relying on manual feature engineering is a strategy that is especially appealing if large amounts of unlabelled data are available. In semi-supervised learning features are extracted from the main input (here: images) using unsupervised learning if target labels are unavailable (Keng 2017). Hence, semi-supervised learning promises better results by using not only labeled samples but also unlabeled data points of partially labeled datasets.\\
\\
In the previous chapter, methods and results for unsupervised learning are presented. One example are convolutional autoencoders. In this section it is shown how convolutional autoencoders with additional soft constraints in the loss function can be used for semi supervised learning. Instead of computing another dataset of sparse representations that contains the results of unsupervised methods, in semi-supervised learning sparse representations are retrieved and mapped to latent features at the same time (Keng 2017). Bottleneck layer activations represent automatically extracted features. For semi-supervised learning one tries to enforce that latent layer activations of autoencoders correlate with the target categories. \\
\\
The network structure was derived from the convolutional autoencoder used for unsupervised learning. The feedforward CNN was replaced by a network that has proven to be suitable to detect at least some features with sufficient adequacy when trained on heads (see 4.1.2). It comprises three convolutional layers with a kernel size of two in the first and three in subsequent layers as well as 32 kernels each. A max pooling layer with stride two is added mainly to reduce the number of total neurons while maintaining a high number of kernels. Additionally, a dropout layer is added to avoid overfitting. In contrast to other implementations for semi-supervised learning (see Keng 2017)  we used the same network for the prediction of labels (when they exist for the current batch) and endcoder for reconstructing images. For the decoder we chose the same one as in the variational autoencoder presented in the previous chapter. The effects of a bypass layer were tested that contains neurons not being subject to the label layer loss (see below). Two variations of the named network were tested: A convolutional variational autoencoder and a variational autoencoder for semi-supervised learning.\\
\\
A challenge resulted from training with multiple inputs (images and labels). As deep learning frameworks (here we employed Keras) usually require a connected graph that links inputs to outputs (here: images), a trick had to be used. A dummy layer was introduced where all information that stems from the labels was multiplied by zero. The output vector was concatenated with the bottleneck of the encoder. As it contains no variance, training and more importantly validation accuracies remain unaffected even though information about the categories that are predicted is added on the input side. Nevertheless the labels are part of the network graph and could hence be used in the loss function.  \\
\\
A custom conditional loss function was used. If labels are present for the current batch the costum loss is equal to a combined loss that comprises the reconstruction loss and the label loss. Here, reconstruction loss refers to the pixelwise loss that was used for the main task of the network - namely mapping of input images back to the same images (fed into the netowork as target values to the output layer). The label loss is used with the goal of mapping label layer activations to the actual labels. It is low if activations in the sigmoid transformed label layer match the target values i.e. the sum of the error layer is low. The loss that is due to labels was multiplied by a custom factor (k). In addition it was defined such that it scales with the current pixel wise reconstruction loss and converges to a constant (c). These values were chosen aiming for an increase of the contribution of the label loss to the combined loss especially in late stages of training.\\

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.8]{Figures/chapter04/vae}
	\decoRule
	\caption[??]{??}
	\label{fig:VAE}
\end{figure}
\\
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.8]{Figures/chapter04/vae_2}
	\decoRule
	\caption[??]{??}
	\label{fig:VAE2}
\end{figure}
\\

The results for the semi-supervised variational autoencoder are illustrated in table (X) and figure X. As one can immediately see, the feature “thin” was adequately mapped to a decisive region in latent space. For the other features, no such clear cut clustering is visible. Reconstructions indicate that the main purpose of the network of predicting the input images was accomplished successfully although the reconstructions look rather uniform. Values for accuracy and sensitivity indicate only poor performance. Only the features “rusty body”(se=.42), “thin”(se=.54) and “bent”(se=.1) sensitivities are above zero. \\
\\

\begin{tabular}{lrrrrrr}
\toprule
{} &  False positive &  False negative &  True positive &  True negative &  Sensitivity &  Specificity \\
\midrule
is\_bended     &            0.04 &            0.37 &           0.04 &           0.55 &         0.10 &         0.94 \\
is\_violet     &            0.00 &            0.08 &           0.00 &           0.92 &         0.00 &         1.00 \\
has\_rost\_body &            0.14 &            0.27 &           0.20 &           0.39 &         0.42 &         0.73 \\
short         &            0.00 &            0.02 &           0.00 &           0.98 &         0.00 &         1.00 \\
thick         &            0.00 &            0.07 &           0.00 &           0.93 &         0.00 &         1.00 \\
thin          &            0.00 &            0.14 &           0.16 &           0.70 &         0.54 &         0.99 \\
\bottomrule
\end{tabular}
\\
Compared to the variational convolutional autoencoder for semi-supervised learning, the convolutional autoencoder for supervised learning performs better, but performance could be improved further. In table (X) the results are summarized. Violet detection was not successful at all (se=.0). For the other features the network was trained on, mediocre results were achieved. Thickness detection showed little sensitivity (se=.04) however a high specificity of 1.0. Better results exist for curvature (se=.18) “rusty body” (se=.42) and “short” (se=.6). The specificity for the “rusty body” is lower (sp=.6) as compared to named other features (se=1 and se=.96). Thin pieces were detected in 76\% of all cases and few false positives characterize the detection of named feature (sp=.97). \\
\\
The approach for semi-supervised learning presented here faces two challenges. First, it should be mentioned that the networks for semi supervised learning were trained only on one perspective although labels were attributed per asparagus piece. Information that is visible on one or two out of three perspectives only can not be mapped to the desired target category. Imagewise labels would be desirable to improve the approach. \\
\\
Second, reconstructions using convolutional autoencoders contain little detail. However, small details in the image, such as small brown spots, play an important role in the classification by human coders. These features are not sufficiently reconstructed by the variational autoencoder. Arguably they are not reflected in the sparse representation that corresponds to latent layer activations. One may speculate that a substantial increase of the network size would help to reconstruct more details and hence extract more features. As generative adversarial networks are known to generate more detailed images (Bao et al. 2017) they could possibly be adopted for semi supervised learning with greater success. However, this is a question that must be answered empirically. \\
\\
In summary, one may conclude that autoencoders appear as an alternative to manual feature engineering if a large dataset is available and only a subset contains labelled samples. However, more research is necessary to find best suitable network structures.\\
\\
\begin{tabular}{lrrrrrr}
\toprule
{} &  False positive &  False negative &  True positive &  True negative &  Sensitivity &  Specificity \\
\midrule
is\_bended     &            0.02 &            0.34 &           0.07 &           0.57 &         0.18 &         0.96 \\
is\_violet     &            0.00 &            0.08 &           0.00 &           0.92 &         0.00 &         1.00 \\
has\_rost\_body &            0.23 &            0.19 &           0.28 &           0.30 &         0.59 &         0.57 \\
short         &            0.00 &            0.01 &           0.01 &           0.97 &         0.57 &         1.00 \\
thick         &            0.00 &            0.06 &           0.00 &           0.93 &         0.04 &         1.00 \\
thin          &            0.02 &            0.07 &           0.23 &           0.68 &         0.76 &         0.97 \\
\bottomrule
\end{tabular}
