{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osnabr√ºck University - Machine Learning (Summer Term 2016) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet is supposed to last two weeks and thus should be solved and handed in before the end of **Sunday, May 22, 2016**. If you need help (and Google and other resources were not enough), feel free to contact your groups designated tutor or whomever of us you run into first. Please upload your results to your group's studip folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Zip files\n",
    "\n",
    "We provided some zip files for these exercises. Once you downloaded them, just put them alongside this sheet and run the following cell to extract them. This allows us to avoid path problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def extract_zip(filename):\n",
    "    \"\"\"\n",
    "    Extracts a zip file of name filename.zip to the current working directory.\n",
    "    \"\"\"\n",
    "    filename = \"{}.zip\".format(filename)\n",
    "    try:\n",
    "        with open(filename, 'rb') as f:\n",
    "            print(\"Extracting {}...\".format(filename))\n",
    "            z = zipfile.ZipFile(f)\n",
    "            for name in z.namelist():\n",
    "                z.extract(name)\n",
    "            print(\"Extracted {}.\".format(filename))\n",
    "    except FileNotFoundError:\n",
    "        print(\"{} was not found.\".format(filename))\n",
    "\n",
    "\n",
    "for filename in ['eigenfaces', 'leafsnap']:\n",
    "    extract_zip(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### SciPy, scikit-learn and pillow\n",
    "\n",
    "From now on you will sometimes need the python package [scikit-learn](https://pypi.python.org/pypi/scikit-learn) (scikit-learn) which depends on [scipy](https://pypi.python.org/pypi/scipy). Another package to handle images is [pillow](https://pypi.python.org/pypi/pillow), which we also recommend to use now (matplotlib already has basic image capabilities, but can only deal with `.png` files). To check if you already have running versions of these packages installed, run the following cell. If something is not found, try to follow the installation steps below. Otherwise just skip the following paragraphs and continue with the assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "assert importlib.util.find_spec('scipy') is not None, 'scipy not found'\n",
    "assert importlib.util.find_spec('sklearn') is not None, 'scikit-learn not found'\n",
    "assert importlib.util.find_spec('PIL') is not None, 'pillow not found'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Unix\n",
    "\n",
    "On Unix systems you can easily install the packages with `pip3 install scipy scikit-learn pillow` from any terminal window. If it fails, try to figure out how to install a Fortran compiler for your OS or ask one of your fellow tutors for help.\n",
    "\n",
    "#### Windows\n",
    "On Windows it is a little bit more difficult to get a Fortran compiler (and although [MinGW](http://www.mingw.org/) offers one it is still very difficult to get everything to run), so we recommend you to take the [precompiled binaries](http://www.lfd.uci.edu/~gohlke/pythonlibs/) of Christoph Gohlke for [scipy](http://www.lfd.uci.edu/~gohlke/pythonlibs/#scipy). If you previously installed a 32bit version of Python download `scipy-0.17.1-cp35-cp35m-win32.whl`, if you have a 64bit version please resort to `\n",
    "scipy-0.17.1-cp35-cp35m-win_amd64.whl`. If you are unsure which version you run, run the following cell to figure it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "print('You are running a {} ({}) version.'.format(*platform.architecture()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To install the binaries open your command line, navigate to your folder where you downloaded the `*.whl` file to (`cd FOLDER`) and run `pip install scipy-0.17.1-cp35-cp35m-win32.whl` (or `pip install scipy-0.17.1-cp35-cp35m-win_amd64.whl` if you downloaded the 64 bit version)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There might be a problem with your `numpy` version - `scipy` needs the Intel Math Kernel Library which is not easily compiled manually.\n",
    "If you run into troubles, uninstall `numpy` with `pip uninstall numpy` and download the `*.whl` files from [Gohlke's website](http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy) again. Proceed as you did with scipy.\n",
    "\n",
    "For most other packages `pip install [package]`, e.g. `pip install pillow` should be enough - however, the website mentioned above provides most precompiled binaries and if you run into troubles with the normal installations take a look there.\n",
    "\n",
    "In case of any other problems, get in touch with your tutors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Assignment 1: Curse of Dimensionality [6 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For the following exercise, be detailed in your answers and provide some examples. Think about keywords like: random vectors in high dimensional space, manifolds and Bertillonage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What are the curse of dimensionality and its implication for pattern classification? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Curse of dimensionality describes the phenomenon that in high dimensional vector spaces, two randomly drawn vectors will almost always be close to orthogonal to each other. This is a real problem in data mining problems, where for a higher number of features, the number of possible combinations and therefore the volume of the resulting feature space exponentionally increases.\n",
    "\n",
    "In such a high dimensional space, data vectors from real data sets lie far away from each other (which means dense sampling becomes impossible, as there aren't enough samples close to each other). This also leads to the problem that pairs of data vectors have a high probability of having similar distances and to be close to orthogonal to each other. The result is that clustering becomes really difficult, as the vectors more or less stand on their own and distance measures cannot be applied easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Explain how this phenomenom could be used to one's advantage."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is actually an advantage if you want to discriminate between a high number of individuals (see Bertillonage, where using only 11 features results in a feature space big enough to discriminate humans), but if you want to get usable information out of data, such a 'singling out' of samples is a great disadvantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Explain in your own words the concepts of descriptive and intrinsic dimensionality."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Intrinsic dimensionality exists in contrast to the descriptive dimensionality of data, which is defined by the numbers of parameters used to produce or represent the raw data (i.e. the number of pixels in an unprocessed image).\n",
    "\n",
    "Additionally to this representive dimensionality, there is also a (most of the time smaller) number of independent parameters which is necessary to describe the data, always in regard to a specific problem we want to use the data on. \n",
    "For example: a data set might consist of a number of portraits, all with size 1920x1080 pixels, which constitutes their descriptive dimensionality. To do some facial recognition on these portraits however, we do not need the complete descriptive dimension space (which would be way too big anyway), but only a few independent parameters (which we can get by doing PCA and looking at the eigenfaces). \n",
    "This is possible because the data never fill out the entire high dimensional vector space but instead concentrate along a manifold of a much lower dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Implement and Apply PCA [8 Points]\n",
    "\n",
    "In this assignment you will implement PCA from the ground up and apply it to the `cars` dataset (simplified from the JSE [2004 New Car and Truck Data](http://www.amstat.org/publications/jse/jse_data_archive.htm)). This dataset consists of measurements taken on 97 different cars. The eleven features measured are: Suggested retail price (USD), Price to dealer (USD), Engine size (liters), Number of engine cylinders, Engine horsepower, City gas mileage, Highway gas mileage, Weight (pounds), Wheelbase (inches), Length (inches) and Width (inches). \n",
    "\n",
    "We would like to visualize these high dimensional features to get a feeling for how the cars relate to each other so we need to find a subspace of dimension two or three into which we can project the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: Load the cars dataset in cars.csv .\n",
    "cars = np.loadtxt('cars.csv', delimiter=',')\n",
    "\n",
    "assert cars.shape == (97, 11), \"Shape is not (97, 11), was {}\".format(cars.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step we need to normalize the data such that they have a zero mean and a unit standard deviation. Use the standard score for this:\n",
    "$$\\frac{X - \\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: Normalize the data and store it in cars_norm.\n",
    "cars_norm = (cars - np.mean(cars, axis=0)) / np.std(cars, axis=0)\n",
    "\n",
    "# Alternatively one could use:\n",
    "# import sklearn.preprocessing\n",
    "# cars_norm = sklearn.preprocessing.scale(cars)\n",
    "\n",
    "assert cars_norm.shape == (97, 11), \"Shape is not (97, 11), was {}\".format(cars.shape)\n",
    "assert np.abs(np.sum(cars_norm)) < 1e-10, \"Absolute sum was {} but should be close to 0\".format(np.abs(np.sum(cars_norm)))\n",
    "assert np.abs(np.sum(cars_norm ** 2) / cars_norm.size - 1) < 1e-10, \"The data is not normalized, sum/N was {} not 1\".format(np.sum(cars_norm ** 2) / cars_norm.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA finds a subspace that maximizes the variance by determining the eigenvectors of the covariance matrix. So we need to calculate the autocovariance matrix and afterwards the eigenvalues. When the data is normalized the autocovariance is calculated as\n",
    "$$C = X\\cdot X^T$$\n",
    "with $X$ being an $n \\times m$ matrix with $n$ features and $m$ samples.\n",
    "The entry $c_{i,j}$ in $C$ tells you how much feature $i$ correlates with feature $j$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: Compute the autocovariance matrix and store it into autocovar\n",
    "autocovar = cars_norm.T @ cars_norm\n",
    "\n",
    "assert autocovar.shape == (11, 11)\n",
    "\n",
    "# TODO: Compute the eigenvalues und eigenvectors and store them into eigenval and eigenvec\n",
    "#       (Figure out a function to do this for you)\n",
    "eigenval, eigenvec = np.linalg.eigh(autocovar)\n",
    "\n",
    "# Alternatively, np.linalg.eig solves the eigenvector problem for general matrices, while eigh\n",
    "# only solves it for symmetric/hermitian matrices. (The auto-covariance matrix is always symmetric.)\n",
    "\n",
    "# eigenval, eigenvec = np.linalg.eig(autocovar)\n",
    "\n",
    "# If you wanted to use the singular value decomposition (SVD), you would have to use the centered \n",
    "# version of cars:\n",
    "\n",
    "# u, s, v = np.linalg.svd(cars-np.mean(cars, 0))\n",
    "# eigenval, eigenvec = s ** 2, v\n",
    "\n",
    "# However, this method usually leads to different results (there are some numerical issues).\n",
    "# For a more detailed overview over different methods of PCA check the file \"PCA Comparison.ipynb\".\n",
    "# In most cases these difference don't matter much, but it is always wise to handle your results\n",
    "# with a certain critical eye.\n",
    "\n",
    "# Sort eigenvectors (and -values) by descending order of eigenvalues.\n",
    "sort = np.argsort(-eigenval)\n",
    "eigenval = eigenval[sort]\n",
    "eigenvec = eigenvec[:,sort]\n",
    "\n",
    "# To get an idea of the eigenvalues we plot them.\n",
    "figure = plt.figure('Eigenvalue comparison')\n",
    "plt.bar(np.arange(len(eigenval)), eigenval)\n",
    "\n",
    "assert eigenval.shape == (11,)\n",
    "assert eigenvec.shape == (11, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should have a matrix full of eigenvectors. We can now do two things: project the data down onto the two dimensional subspace to visualize it and we can also plot the two first principle component vectors as eleven two dimensional points to get a feeling for how the features are projected into the subspace. Execute the cells below and describe what you see. Is PCA a good method for this problem? Was it justifiable that we only considered the first two principle components? What kinds of cars are in the four quadrants of the first plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project the data down into the two dimensional subspace\n",
    "proj = cars_norm @ eigenvec[:,0:2]\n",
    "\n",
    "# Plot projected data\n",
    "fig = plt.figure('Data projected onto first two Principal Components')\n",
    "fig.gca().set_xlim(-8, 8)\n",
    "fig.gca().set_ylim(-4, 7)\n",
    "plt.scatter(proj[:,0], proj[:,1])\n",
    "# Divide plot into quadrants\n",
    "plt.axhline(0, color='green')\n",
    "plt.axvline(0, color='green')\n",
    "# force drawing on 'run all'\n",
    "fig.canvas.draw()\n",
    "\n",
    "# Plot eigenvectors\n",
    "eig_fig = plt.figure('Eigenvector plot')\n",
    "plt.scatter(eigenvec[:,0], eigenvec[:,1])\n",
    "\n",
    "# add labels\n",
    "labels = ['Suggested retail price (USD)', 'Price to dealer (USD)', \n",
    "          'Engine size (liters)', 'Number of engine cylinders', \n",
    "          'Engine horsepower', 'City gas mileage' , \n",
    "          'Highway gas mileage', 'Weight (pounds)', \n",
    "          'Wheelbase (inches)', 'Length (inches)', 'Width (inches)']\n",
    "for label, x, y in zip(labels, eigenvec[:,0], eigenvec[:,1]):\n",
    "    plt.annotate(\n",
    "        label, xy = (x, y), xytext = (-20, 20),\n",
    "        textcoords = 'offset points', ha = 'left', va = 'bottom',\n",
    "        bbox = dict(boxstyle = 'round,pad=0.5', fc = 'blue', alpha = 0.5),\n",
    "        arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
    "# force drawing on 'run all'\n",
    "eig_fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA is good\n",
    "The first plot shows the complete dataset projected down onto the two first principle components. Only few points overlap and the points are generally spread out well in the subspace. There is not much trend in the plot which is what we desired, i.e. the axes are not redundant. No clusters can be recognized. \n",
    "\n",
    "#### The first two PCs are good\n",
    "It is admissible to pick two dimensions, although only the first eigenvector has a very high magnitude in comparison. \n",
    "\n",
    "The 1D plot, which might be better if we take the eigenvalues into account, yields little information in the data on a visual level. However, it gives a better idea of how the original features will be distributed in the space.\n",
    "\n",
    "The 3D plot is already very hard to grasp by just taking a look at it. So 2D seems to be a good choice.\n",
    "\n",
    "In general there are several different strategies to decide the number of dimensions onto which you want to project your data. Here is a short overview over just a few common choices:\n",
    "\n",
    "- Eigenvalue magnitudes: Find the cut-off depth. This is useful for classification problems, especially\n",
    "  for problems to be solved by computers.\n",
    "- Visualization: Choose the number of dimensions which is useful to visualize the data in a meaningful way. This\n",
    "  choice depends a lot on your problem definition. For printing 2D is usually a good choice - but maybe your data\n",
    "  is just very nice for !D already. Or maybe you are using a glyph plot (see sheet 06) which can represent high \n",
    "  dimensional data.\n",
    "- Classification results: In the Eigenfaces assignment below we figured out that the number of principal \n",
    "  components (and thus the number of dimensions) can have a crucial impact on classification rates. It is thus\n",
    "  an option to fine tune the number of dimensions for a good classification result on some training set.\n",
    "\n",
    "#### Interpretation of the plot is very subjective\n",
    "Let's first take a look at the second plot. Each of the eleven points denotes one of the original base vectors. If you would draw arrows from the origin to each of the eleven points, you would draw projections of the original axes.\n",
    "\n",
    "To give it a little bit more meaning, let's investigate this further. Let's take a car with six cylinders and all other values on average. We can change the number of cylinders and see how it moves along the cylinder axis (blue arrow). \n",
    "\n",
    "As you can see, the six cylinder car is close to the average (i.e. close to the center), while the eight cylinder car is further away in the positive cylinder direction and the two cylinder car is further away in the negative direction.\n",
    "\n",
    "If we now increase (or decrease) the city gas mileage of the eight cylinder car by 30%, the car will move along the mileage axis (orange arrow).\n",
    "\n",
    "The dashed arrows indicate the linear combination of the two features for the eight cylinder car with higher city gas mileage. You can see how they are parallel to the respective axes.\n",
    "\n",
    "Taking all that into account we can say:\n",
    "\n",
    "- **Top right**: Cars with high gas mileages. This might be limousines.\n",
    "- **Bottom right**: Cars with low prices and low power, but average sizes and higher gas mileages. This might also be \n",
    "  limousines, but smaller ones.\n",
    "- **Bottom left**: Cars with big measurements and average pricing. This might be family cars.\n",
    "- **Top left**: Cars with considerably high power and prices which are still light and small.\n",
    "  This might be sports cars.\n",
    "\n",
    "Note that this interpretation is just describing the general trend. Due to the nature of linear combinations, it is easily possible to come up with a car which has some exceptional values which lead to cancellation of others.\n",
    "\n",
    "Note also that depending on the method used to calculate the eigenvectors, your axes and thus your interpretation might slightly differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "means = np.mean(cars, 0)\n",
    "stds = np.std(cars, 0)\n",
    "\n",
    "six_cylinders = np.copy(means)\n",
    "six_cylinders[3] = 6\n",
    "# we have to use the same normalization as we used with the data set, so we also need to scale it\n",
    "six_cylinders = (six_cylinders - means) / stds\n",
    "\n",
    "eight_cylinders = np.copy(means)\n",
    "eight_cylinders[3] = 8\n",
    "eight_cylinders = (eight_cylinders - means) / stds\n",
    "\n",
    "two_cylinders = np.copy(means)\n",
    "two_cylinders[3] = 2\n",
    "two_cylinders = (two_cylinders - means) / stds\n",
    "\n",
    "eight_cylinders_more_mileage = np.copy(means)\n",
    "eight_cylinders_more_mileage[3] = 8\n",
    "eight_cylinders_more_mileage[5] *= 1.3\n",
    "eight_cylinders_more_mileage = (eight_cylinders_more_mileage - means) / stds\n",
    "\n",
    "eight_cylinders_fewer_mileage = np.copy(means)\n",
    "eight_cylinders_fewer_mileage[3] = 8\n",
    "eight_cylinders_fewer_mileage[5] *= 0.7\n",
    "eight_cylinders_fewer_mileage = (eight_cylinders_fewer_mileage - means) / stds\n",
    "\n",
    "figure = plt.figure('Feature explanations')\n",
    "for label, car in [('6 Cyl', six_cylinders), \n",
    "                   ('8 Cyl', eight_cylinders), \n",
    "                   ('2 Cyl', two_cylinders), \n",
    "                   ('8 Cyl/high Mile', eight_cylinders_more_mileage),\n",
    "                   ('8 Cyl/low Mile', eight_cylinders_fewer_mileage)\n",
    "                  ]:\n",
    "    plt.plot(*zip(car @ eigenvec[:, 0:2]), label=label, linestyle='none', marker='o')\n",
    "\n",
    "plt.axhline(0, c='g', alpha=0.2)\n",
    "plt.axvline(0, c='g', alpha=0.2)\n",
    "plt.arrow(0, 0, eigenvec[3, 0], eigenvec[3, 1], alpha=0.2, color='blue')\n",
    "plt.arrow(0, 0, eigenvec[5, 0], eigenvec[5, 1], color='orange', alpha=0.2)\n",
    "\n",
    "data_ec = eight_cylinders @ eigenvec[:, 0:2]\n",
    "data_ecmm = eight_cylinders_more_mileage @ eigenvec[:, 0:2]\n",
    "plt.arrow(*data_ec, *(data_ecmm - data_ec), linestyle='dashed', length_includes_head=True, color='r', alpha=0.8)\n",
    "plt.arrow(0, 0, *data_ec, linestyle='dashed', length_includes_head=True, color='r', alpha=0.8)\n",
    "\n",
    "plt.legend(loc=3, prop={'size':12})\n",
    "figure.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "fig = plt.figure('PCA dimension comparison on the cars data', figsize=(11,13))\n",
    "\n",
    "plt.subplot(3,2,1)\n",
    "data = cars_norm @ eigenvec[:, 0]\n",
    "plt.scatter(data, np.zeros(data.shape))\n",
    "plt.axhline(0, color='green')\n",
    "plt.title('Data projected into 1D')\n",
    "\n",
    "plt.subplot(3,2,2)\n",
    "plt.scatter(eigenvec[:, 0], np.zeros(eigenvec[:, 0].shape), marker='x', c='r')\n",
    "plt.axhline(0, color='green')\n",
    "for label, x, y in zip(labels, eigenvec[:, 0], np.zeros(eigenvec[:, 0].shape)):\n",
    "    plt.annotate(\n",
    "        label, xy = (x, y), xytext = (-20, 20),\n",
    "        textcoords = 'offset points', ha = 'left', va = 'bottom',\n",
    "        bbox = dict(boxstyle = 'round,pad=0.5', fc = 'blue', alpha = 0.2),\n",
    "        arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
    "plt.title('Original axes projected into 1D')\n",
    "\n",
    "plt.subplot(3,2,3)\n",
    "plt.scatter(proj[:,0], proj[:,1])\n",
    "plt.axhline(0, color='green')\n",
    "plt.axvline(0, color='green')\n",
    "plt.title('Data projected into 2D')\n",
    "\n",
    "plt.subplot(3,2,4)\n",
    "plt.scatter(eigenvec[:,0], eigenvec[:,1])\n",
    "plt.axhline(0, color='green')\n",
    "plt.axvline(0, color='green')\n",
    "for label, x, y in zip(labels, eigenvec[:,0], eigenvec[:,1]):\n",
    "    plt.annotate(\n",
    "        label, xy = (x, y), xytext = (-20, 20),\n",
    "        textcoords = 'offset points', ha = 'left', va = 'bottom',\n",
    "        bbox = dict(boxstyle = 'round,pad=0.5', fc = 'blue', alpha = 0.2),\n",
    "        arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
    "plt.title('Original axes projected into 2D')\n",
    "\n",
    "ax3d = plt.subplot(3,2,5, projection='3d')\n",
    "data = cars_norm @ eigenvec[:, 0:3]\n",
    "ax3d.scatter(data[:,0], data[:,1], zs=data[:,2])\n",
    "plt.title('Data projected into 3D')\n",
    "\n",
    "ax3d = plt.subplot(3,2,6, projection='3d')\n",
    "ax3d.scatter(eigenvec[:, 0], eigenvec[:, 1], zs=eigenvec[:, 2], marker='x', c='r')\n",
    "# Projection is not updated when the plot changes, so remove this loop if you want to explore!\n",
    "for label, x, y, z in zip(labels, eigenvec[:, 0], eigenvec[:, 1], eigenvec[:, 2]):\n",
    "    x, y, z = proj3d.proj_transform(x, y, z, ax3d.get_proj())\n",
    "    plt.annotate(\n",
    "        label, xy = (x, y), xytext = (-20, 20),\n",
    "        textcoords = 'offset points', ha = 'left', va = 'bottom',\n",
    "        bbox = dict(boxstyle = 'round,pad=0.5', fc = 'blue', alpha = 0.2),\n",
    "        arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
    "plt.title('Original axes projected into 3D')\n",
    "        \n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Assignment 3: Eigenfaces [6 Points]\n",
    "\n",
    "A very famous example of applying PCA to solve classification problems are Eigenfaces. It employs PCA to project training images of faces into the Eigenspace to reduce the dimensionality of those images by a few magnitudes. To classify test images they are projected into the Eigenspace and the (euclidean) distance between the sample and each reduced training image is calculated - the closest training image wins.\n",
    "\n",
    "To avoid problems with your own implementation of PCA in assignment 2 you will now rely on the `scipy` package which already provides an implementation of PCA.\n",
    "\n",
    "First you will implement the Eigenfaces algorithm and apply it to a subset of the [Extended Yale Face Database B](http://www.cad.zju.edu.cn/home/dengcai/Data/FaceData.html) (the subset of files is stored in `eigenfaces.zip`). Then you will apply the same procedure to another data set, [leafsnap](http://leafsnap.com/dataset/) (`leafsnap.zip`). \n",
    "\n",
    "The face dataset is provided in two sets (`eigenfaces/train` and `eigenfaces/test`) which each yield four images per each of five different people in the [pgm](http://netpbm.sourceforge.net/doc/pgm.html) format (which is supported by `matplotlib` via `pillow`). \n",
    "The leafsnap dataset is provided in two sets as well (`leafsnap/train` and `leafsnap/test`) which each contain two images of leaves from 184 different tree species. They were automatically cropped to square images, resized to a resolution of 200x200 pixels and converted to the [png](https://en.wikipedia.org/wiki/Portable_Network_Graphics) format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### a) Understanding PCA for Eigenfaces\n",
    "\n",
    "To apply PCA on the samples you first need to understand what we are trying to do.\n",
    "\n",
    "Usually a grayscale image is seen as a set of pixels each being a three dimensional datapoint consisting of features X coordinate, Y coordinate and gray value. This is very useful for many types of segmentation or clustering problems, where our image is a space with three dimensions (X, Y, Color).\n",
    "\n",
    "For the Eigenface method we have to consider images in a different way!\n",
    "Here each image is one data point in the hyper space containing all possible images with the same resolution. So each image is a data point consisting of pixel many dimensions or features, each having a value of the color at that pixel.\n",
    "\n",
    "To make this a little bit more clear, consider the 2x2 binary images (see the plot below). Each image is one out of 16 possible data points in the space of all possible 2x2 binary images. The space has four dimensions with two possible values in each, so there are $2^4=16$ possible images. They can be described as feature vectors: \n",
    "$$\\text{image} = \\left(\\matrix{\\text{pixel color top left}\\\\ \\text{pixel color top right}\\\\ \\text{pixel color bottom left}\\\\ \\text{pixel color bottom right}}\\right)$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creates all possible combinations of 4 0s and 1s.\n",
    "images = list(itertools.product([0,1], repeat=4))\n",
    "\n",
    "bin_images = plt.figure('Binary 2x2 Images')\n",
    "for i, img in enumerate(images):\n",
    "    ax = plt.subplot(4, 4, i + 1)\n",
    "    # Remove axis ticks for a better image experience.\n",
    "    ax.axes.get_xaxis().set_ticks([])\n",
    "    ax.axes.get_yaxis().set_ticks([])\n",
    "    # Pretend the artificial arrays are beautiful pictures.\n",
    "    plt.imshow(np.array([img] * 3).T.reshape(2,2,3), interpolation='nearest')\n",
    "# force drawing on 'run all'\n",
    "bin_images.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is a relatively small space. What if there were 256 different gray values? The number of possible images increased to $256^4=4,294,967,296$.\n",
    "\n",
    "Our eigenface images are 192x168 pixels each and allow 256 different gray values. This means there are $256^{192 \\cdot 168}=256^{32256}$ possible images (which is a lot as you can see below - a number with $77681$ digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "a = 256 ** (192 * 168)\n",
    "print(\"Digits: {}\".format(math.ceil(math.log(a) / math.log(10))))\n",
    "# print(a) # This line takes a second or two to execute, be warned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How many principal components are there at most when you apply the PCA with the 20 training face images provided?\n",
    "How many principal components were there for the 16 binary images if we made a PCA on all of them?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are at most 20 principal components possible for the face images, but only four principal components for the binary images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### b) Reading the data\n",
    "\n",
    "Implement the methods `get_sample_database` and `read_sample` in the following cells to easily read in all images we are going to work with. Use the function `get_class_name_from_file` to generate class names to store as `name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_class_name_from_file(file):\n",
    "    \"\"\"\n",
    "    Returns a class name from the file path provided.\n",
    "    Takes only the filename (discards the rest of the path), \n",
    "    splits it on underscores and discards the last parts (assumed \n",
    "    to be numbers and file endings). \n",
    "    Then the parts are joined again with spaces and the string is\n",
    "    capitalized.\n",
    "    For example\n",
    "    /Users/esinclair/work/dino_01_1.jpg\n",
    "    will result in\n",
    "    Dino 01\n",
    "    \n",
    "    Args:\n",
    "        file    the filename to operate on\n",
    "    Returns:\n",
    "        A class name derived from the filename.\n",
    "    \"\"\"\n",
    "    return ' '.join(file.split(os.path.sep)[-1][:-4].split('_')[0:2]).capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_sample(imagepath):\n",
    "    \"\"\"\n",
    "    Reads an image file using plt.imread.\n",
    "    If the image has multiple color channels, only the first \n",
    "    channel is returned.\n",
    "    \n",
    "    Args:\n",
    "        imagepath   the path to the image file\n",
    "    Returns:\n",
    "        A two dimensional np array with the color information.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    img = plt.imread(imagepath)\n",
    "    if len(img.shape) > 2:\n",
    "        return img[:,:,0]\n",
    "    return img\n",
    "#     A more pythonic way to do it would be to use a try/except pattern:\n",
    "#     try:\n",
    "#         return img[:,:,0]\n",
    "#     except IndexError:\n",
    "#         return img\n",
    "\n",
    "\n",
    "face = read_sample(os.path.join('eigenfaces', 'test', 'person_01_1.pgm'))\n",
    "leaf = read_sample(os.path.join('leafsnap', 'test', 'acer_palmatum_1.png'))\n",
    "\n",
    "assert face.shape == (192, 168), 'face.shape does not fit! Was: {}'.format(face.shape)\n",
    "assert leaf.shape == (200, 200), 'leaf.shape does not fit! Was: {}'.format(leaf.shape)\n",
    "\n",
    "del face, leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pprint\n",
    "import numpy as np\n",
    "\n",
    "def get_sample_database(path, limit=-1):\n",
    "    \"\"\"\n",
    "    Returns a list of dictionaries {'name': ..., 'data': ...}, where\n",
    "    names correspond the classification class (e.g. person01) and data corresponds\n",
    "    to the image data from that file.\n",
    "    Since there can be multiple training samples for the same classification, the\n",
    "    entries in 'name' don't need to be unique!\n",
    "    \n",
    "    Args:\n",
    "        path    the path to the images (e.g. the eigenfaces/train directory)\n",
    "        limit   the number of images to be read, values < 0 (default is -1) \n",
    "                allow reading of all images\n",
    "    Returns:\n",
    "        A list of dictionaries containing the image names and data, like:\n",
    "        [{'name': 'Person 01', 'data': np.array(...)}, {'name': 'Person 01', 'data': np.array(...)}, ...]\n",
    "    \"\"\"\n",
    "    # Get all sample file paths.\n",
    "    files = sorted(glob.glob(os.path.join(path, '*')))\n",
    "\n",
    "    # Reduce the number of samples to the limit.\n",
    "    if limit >= 0:\n",
    "        files = files[0:min(limit, len(files))]\n",
    "\n",
    "    # TODO: Create the database list.\n",
    "    database = []\n",
    "    for file in files:\n",
    "        name = get_class_name_from_file(file)\n",
    "        try:\n",
    "            data = read_sample(file)\n",
    "        except OSError as error:\n",
    "            print('Skipping {} (Error: {})'.format(file, error))\n",
    "            continue\n",
    "        # create database entry with name and data\n",
    "        database += [{'name': name, 'data': data}]\n",
    "    return database\n",
    "\n",
    "\n",
    "faces_test = get_sample_database(os.path.join('eigenfaces', 'test'), 2)\n",
    "leafsnap_test = get_sample_database(os.path.join('leafsnap', 'train'), 1)\n",
    "\n",
    "print('Faces:')\n",
    "pprint.pprint(faces_test)\n",
    "print('Leaves:')\n",
    "pprint.pprint(leafsnap_test)\n",
    "\n",
    "assert len(faces_test) == 2, 'faces_test has length {}'.format(len(faces_test))\n",
    "assert len(leafsnap_test) == 1, 'leafsnap_test has length {}'.format(len(leafsnap_test))\n",
    "\n",
    "assert faces_test[0]['name'] == 'Person 01', \"faces_test[0]['name'] was not Person 01 but {}\".format(faces_test[0]['name'])\n",
    "\n",
    "assert isinstance(leafsnap_test[0]['data'], np.ndarray), \"leafsnap_test[0]['data'] is no numpy array\"\n",
    "\n",
    "del faces_test, leafsnap_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### c) Apply PCA\n",
    "\n",
    "To apply the PCA for the Eigenfaces you will implement three more functions. `compose_matrix` and `plot_results` which will be used in addition to the PCA from `sklearn`. Additionally there will be a function which evaluates the success, `report_success`.\n",
    "\n",
    "First implement `compose_matrix`, which creates a raw data matrix out of the databases we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def compose_matrix(database):\n",
    "    \"\"\"\n",
    "    This method creates a raw data matrix from the database provided.\n",
    "    If there are N database entries with an np.array of MxP values as 'data',\n",
    "    then the resulting matrix is of size Nx(M*P).\n",
    "    Thus row 0 contains the pixel values of the 0th element of the database,\n",
    "    row 1 the values of the 1st, etc.\n",
    "    \n",
    "    Args:\n",
    "        database   a database as it is returned by get_sample_database(path, limit)\n",
    "    Returns:\n",
    "        An Nx(M*P) matrix.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return np.stack(db_val['data'].flatten() for db_val in database)\n",
    "\n",
    "\n",
    "# simple example: artificial 2x3 all white and all black images\n",
    "simple_database = [{'name': 'allwhite', 'data': np.array([[1, 1, 1], [1, 1, 1]])}, \n",
    "                   {'name': 'allblack', 'data': np.array([[0, 0, 0], [0, 0, 0]])}]\n",
    "matrix = compose_matrix(simple_database)\n",
    "\n",
    "assert matrix.shape == (2, 6), \"The shape should be (2, 6), was: {}\".format(matrix.shape)\n",
    "assert np.all(matrix[0,:]), \"The first row should be only 1s, was: {}\".format(matrix[0,:])\n",
    "assert not np.any(matrix[1,:]), \"The second row should be only 0s, was: {}\".format(matrix[1,:])\n",
    "\n",
    "# complex example: actual images\n",
    "complex_database = get_sample_database(os.path.join('eigenfaces', 'train'), 4)\n",
    "matrix = compose_matrix(complex_database)\n",
    "\n",
    "assert matrix.shape == (4, 32256), \"The shape should be (4, 32256), was: {}\".format(matrix.shape)\n",
    "\n",
    "del simple_database, complex_database, matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next implement `report_success`, a function which returns the success rate and counts for hits and misses. To check if a sample was correctly identified, check the names of the train and test samples for equality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def report_success(train_database, test_database, best_matches):\n",
    "    \"\"\"\n",
    "    This function returns the number of hits and misses.\n",
    "    Takes a train database and a test database as returned by \n",
    "    get_sample_database(path, limit) and a list of best_matches.\n",
    "    \n",
    "    The test database entries are mapped to the train database entries via \n",
    "    the best matches array.\n",
    "    That means for each entry in the test database there has to be a single\n",
    "    integer value in the best matches list which corresponds to the index of\n",
    "    the match for that test sample in the train database.\n",
    "    \n",
    "    Args:\n",
    "        train_database  the training database\n",
    "        test_database   the test database\n",
    "        best_matches    a list of indices (len(best_matches) == len(test_database))\n",
    "                        to map the test samples to their best matching training \n",
    "                        samples\n",
    "    Returns:\n",
    "        Hits, Misses, Ratio\n",
    "            counts for hits and misses, respectively and the ratio hits/total\n",
    "    \"\"\"\n",
    "    hits = sum(train_database[best_matches[index]]['name'] == test_sample['name'] for index, test_sample in enumerate(test_database))\n",
    "    return hits, len(test_database) - hits, hits / len(test_database)\n",
    "\n",
    "\n",
    "train_data = [{'name': 'hit', 'data': None}, \n",
    "              {'name': 'hit', 'data': None},\n",
    "              {'name': 'hit', 'data': None},\n",
    "              {'name': 'success', 'data': None}]\n",
    "test_data  = [{'name': 'hit', 'data': None}, \n",
    "              {'name': 'miss', 'data': None},\n",
    "              {'name': 'hit', 'data': None},\n",
    "              {'name': 'success', 'data': None},\n",
    "              {'name': 'failure', 'data': None}]\n",
    "best_matches = [0, 1, 1, 3, 3]\n",
    "\n",
    "hits, misses, ratio = report_success(train_data, test_data, best_matches)\n",
    "\n",
    "assert hits == 3, \"Result for hits {} is incorrect, should be 3!\".format(hits)\n",
    "assert misses == 2, \"Result for misses {} is incorrect, should be 2!\".format(misses)\n",
    "assert abs(ratio - 0.6) < 1e-10, \"Result for ratio {} is incorrect, should be 0.6!\".format(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now implement `plot_results`. It should be able to plot at least the first 10 results in one figure (the rest can be skipped), resulting in a subplot with 5 rows and 4 columns, where columns 1 and 3 contain test images and columns 2 and 4 the corresponding result images.\n",
    "\n",
    "*Hint:* Matplotlib's `imshow` will scale (m,n,1) images according to a colormap, thus the results might look a bit different than when you look at the images in your other image programs. To get similar results you can replicate the matrix to shape (m,n,3) - this involves for example `np.tile` and `np.newaxis`, as it can be found on [stackoverflow](http://stackoverflow.com/q/1721802/3004221). (Note that this is not necessary for the exercise, it's enough if you get some images!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_results(train_database, test_database, best_matches, max_results=float('inf')):\n",
    "    \"\"\"\n",
    "    Takes a train database and a test database as returned by \n",
    "    get_sample_database(path, limit) and a list of best_matches.\n",
    "    \n",
    "    Plots at most max_results many results, where the test database entries\n",
    "    are mapped to the train database entries via the best matches array.\n",
    "    That means for each entry in the test database there has to be a single\n",
    "    integer value in the best matches list which corresponds to the index of\n",
    "    the match for that test sample in the train database.\n",
    "    \n",
    "    The results are plotted in subplots of size 5x4, where the 1st and 3rd \n",
    "    column contain the test images and the 2nd and 4th column the corresponding\n",
    "    train images.\n",
    "    \n",
    "    Args:\n",
    "        train_database  the training database\n",
    "        test_database   the test database\n",
    "        best_matches    a list of indices (len(best_matches) == len(test_database))\n",
    "                        to map the test samples to their best matching training \n",
    "                        samples\n",
    "        max_results     the number of results to plot\n",
    "    \"\"\"\n",
    "    for i, test_sample in enumerate(test_database):\n",
    "        if i >= max_results:\n",
    "            return\n",
    "        if i % 10 == 0:\n",
    "            try:\n",
    "                # force drawing on 'run all'\n",
    "                fig.canvas.draw()\n",
    "            except NameError:\n",
    "                pass\n",
    "            fig = plt.figure()\n",
    "        for j, im in enumerate([test_sample, train_database[best_matches[i]]]):\n",
    "            ax = plt.subplot(5, 4, (2 * i) % 20 + j + 1)\n",
    "            ax.axes.get_xaxis().set_ticks([])\n",
    "            ax.axes.get_yaxis().set_ticks([])\n",
    "            plt.imshow(im['data'])\n",
    "            plt.imshow(np.tile(im['data'][:,:,np.newaxis], (1, 1, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally implement the PCA procedure. Follow the `TODO` comments in the code below (the `TODO (bonus exercise)` is optional but provides some interesting insights towards the understanding of principal components in this method).\n",
    "Figure out how to control the number of components used by the PCA. Find the lowest possible values still able to achieve 100% (90%, 80%) correct classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.decomposition.pca import PCA\n",
    "\n",
    "BASE = 'eigenfaces'\n",
    "# BASE = 'leafsnap'\n",
    "TRAIN = os.path.join(BASE, 'train')\n",
    "TEST = os.path.join(BASE, 'test')\n",
    "\n",
    "# Read in the samples for training and test.\n",
    "train_database = get_sample_database(TRAIN)\n",
    "test_database = get_sample_database(TEST)\n",
    "\n",
    "# TODO: Create data matrices for training and test.\n",
    "raw_traindata = compose_matrix(train_database)\n",
    "raw_testdata = compose_matrix(test_database)\n",
    "\n",
    "# TODO: Create an instance of PCA and fit the training data to it.\n",
    "pca = PCA(5) # 5, 4, 2\n",
    "pca.fit(raw_traindata)\n",
    "\n",
    "# TODO (bonus exercise): Retrieve the principal components from the pca\n",
    "#       and plot them as images. You can limit the number of components\n",
    "#       to plot.\n",
    "resolution = train_database[0]['data'].shape\n",
    "n_pc = min(20, len(pca.components_))\n",
    "princomps = [pc.reshape(resolution) for pc in pca.components_[0:n_pc]]\n",
    "fig = plt.figure('Principal Components {}'.format(BASE))\n",
    "for index, pc in enumerate(princomps):\n",
    "    ax = plt.subplot(5, 4, index + 1)\n",
    "    ax.axes.get_xaxis().set_ticks([])\n",
    "    ax.axes.get_yaxis().set_ticks([])\n",
    "    plt.imshow(pc, cmap='Spectral')\n",
    "\n",
    "# TODO: transform the training data into the new PCA base\n",
    "#       and do the same for the test data.\n",
    "traindata = pca.transform(raw_traindata)\n",
    "testdata = pca.transform(raw_testdata)\n",
    "\n",
    "# TODO: Find the best match for each test sample in the training\n",
    "#       samples. (Consider using cdist).\n",
    "best_matches = np.argmin(cdist(testdata, traindata), 1)\n",
    "\n",
    "# TODO: Report the success and plot the results \n",
    "#       (try only 20 results to not get too many plots).\n",
    "print(\"Hits: {}, Misses: {} ({:.0%})\".format(*report_success(train_database, test_database, best_matches)))\n",
    "plot_results(train_database, test_database, best_matches, 20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "PC needed for classification of face images:\n",
    "\n",
    "Classification goal | PC needed \n",
    "--------------------+-----------\n",
    "              100 % | 5\n",
    "               90 % | 4\n",
    "               80 % | 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### d) Eigenleaves\n",
    "\n",
    "In the code above uncomment `# BASE = 'leafsnap'` and run the code again. The calculations will take a little longer as there are more samples involved (remember to only plot a few results). Again try out different numbers of components used by the PCA. Explain the differences of the results between the two datasets."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Eigenfaces only works well under controlled lab scenarios and with heavy manual preprocessing involved. The face images are all pretty similar: eyse, nose, mouth are at similar positions and the lighting is, at least inter-subject-wise, mostly close to constant. The very few variations allow for closely related images.\n",
    "\n",
    "In the leafsnap data however, lab images (training) are often scaled differently than field images (test). The images are often rotated with respect to the lab images. The lighting differes from image to image.\n",
    "\n",
    "Overall the controlled setting and careful preprocessing for the face images are highly important for the success of this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Assignment 4: PCA [4 Points]\n",
    "\n",
    "In this exercise we investigate the statement from the lecture that PCA finds the subspace that captures most of the data variance. To be more precise, we show that the orthonormal projection onto an $m$-dimensional subspace that maximizes the variance of the projected data is defined by the principal components, i.e. by the $m$ eigenvectors of the autocorrelation matrix $C$ corresponding to the $m$ largest eigenvalues. We proceed in two steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### a)\n",
    "\n",
    "First consider a one dimensional subspace: determine a (unit) vector $\\vec{p}$, such that the variance of the data, when projected onto the subspace determined by that vector, is maximal.\n",
    "\n",
    "The autocorrelation matrix $C$ allows to compute the variance of the projected data as $\\vec{p}^{T}C\\vec{p}$. We want to maximize this expression. To avoid $\\|\\vec{p}\\|\\to\\infty$ we will only consider unit vectors, i.e. we constrain $\\vec{p}$ to be normalized: $\\vec{p}^T\\vec{p}=1$. Maximize the expression with this constraint (which can be done using a Lagrangian multiplier). Conclude that a suitable $\\vec{p}$ has to be an eigenvector of $C$ and describe which of the eigenvectors is optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Solution:**\n",
    "We want to maximize the expression\n",
    "$$\\vec{p}^T C\\vec{p} + \\lambda(1-\\vec{p}^T\\vec{p})$$\n",
    "with respect to $\\vec{p}$, i.e. we have to find solutions for\n",
    "$$\\frac{\\partial}{\\partial\\vec{p}}\\left[ \\vec{p}^T C\\vec{p} + \\lambda(1-\\vec{p}^T\\vec{p})\\right] = 0$$\n",
    "This leads to the equation\n",
    "$$C\\vec{p} = \\lambda\\vec{p}$$\n",
    "in other words: for a vector $\\vec{p}$ to maximize our expression, it has to be an eigenvector $C$ and $\\lambda$ has to be the corresponding eigenvalue.\n",
    "By left multiplying with $\\vec{p}^T$ and using the fact that $\\vec{p}^T\\vec{p}=1$, we gain\n",
    "$$\\vec{p}^TC\\vec{p}=\\lambda$$\n",
    "i.e. the projected variance will correspond to the eigenvalue $\\lambda$ and hence is maximized when choosing the largest eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### b)\n",
    "\n",
    "Now proof the statement for the general case of an $m$-dimensional projection space.\n",
    "\n",
    "Use an inductive argument: assume the statement has been shown for the $(m-1)$-dimensional projection space, spanned by the $m-1$ (orthonormal) eigenvectors $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$ corresponding to the $(m-1)$ largest eigenvalues $\\lambda_1,\\ldots,\\lambda_{m-1}$. Now find a (unit) vector $\\vec{p}_m$, orthogonal to the existing vectors $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$, that maximizes the projected variance $\\vec{p}_m^TC\\vec{p}_m$. Proceed similar to case (a), but with additional Lagrangian multipliers to enforce the orthogonality constraint. Show that the new vector $\\vec{p}_m$ is an eigenvector of $C$. Finally show that the variance is maximized for the eigenvector corresponding to the $m$-th largest eigenvalue $\\lambda_m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Solution:** Assume that the result holds for projection spaces of dimensionality $m-1$. We will now show that it then also holds for dimensionality $m$: we consider a subspace spanned by the $m-1$ (orthonormal) eigenvectors $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$ corresponding to the $(m-1)$ largest eigenvalues $\\lambda_1,\\ldots,\\lambda_{m-1}$, and a new vector $\\vec{p}_{m}$ whos properties we will now examine. First, this vector should be linearly independent from $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$, as it should define the new $m$-th dimension. The property can be enforced by the (stronger) requirement that $\\vec{p}_{m}$ should be orthogonal to $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$, i.e. \n",
    "$$\\vec{p}_m^T\\vec{p}_{i}=0 \\text{ for } i=1,\\ldots,m-1,$$\n",
    "which can be expressed using Lagrange multipliers $\\eta_1,\\ldots,\\eta_{m-1}$. As argued in part (a), the variance in direction $\\vec{p}_m$ is given by\n",
    "$$\\vec{p}_{m}^TC\\vec{p}_{m}.$$\n",
    "We want to maximize this value, again with the additional constraint that $\\vec{p}_{m}$ is normalized, i.e.\n",
    "$$\\vec{p}_{m}^T\\vec{p}=1,$$\n",
    "which will be expressed by an additional Lagrange multiplier $\\lambda_M$. So in total we want to maximize the function\n",
    "$$\\vec{p}_{m}^TC\\vec{p}_{m} + \\sum_{i=1}^{m-1}\\eta_i\\vec{p}_m^T\\vec{p}_{i} + \\lambda_m(1-\\vec{p}_{m}^T\\vec{p}_{m})$$\n",
    "with respect to $\\vec{p}_m$, i.e. we have to find solutions for\n",
    "\\begin{align}\n",
    "  0\n",
    "  & = \\frac{\\partial}{\\partial\\vec{p}_m}\\left[\\vec{p}_{m}^TC\\vec{p}_{m} \n",
    "  + \\sum_{i=1}^{m-1}\\eta_i\\vec{p}_m^T\\vec{p}_{i}\n",
    "  + \\lambda_m(1-\\vec{p}_{m}^T\\vec{p}_m)\\right] \\\\\n",
    "  & = 2C\\vec{p}_m + \\sum_{i=1}^{m-1}\\eta_i\\vec{p}_m^T\\vec{p}_{i} - 2\\lambda_m\\vec{p}_{m}\n",
    "\\end{align}\n",
    "Multiplying this equation with $\\vec{p}_{j}^T$ from the left yields (due to the orthogonality constraint)\n",
    "\\begin{align}\n",
    "  0 = \\vec{p}_{j}^T 0\n",
    "  & = \\vec{p}_{j}^T 2C\\vec{p}_m +\n",
    "  \\vec{p}_{j}^T \\sum_{i=1}^{m-1}\\eta_i\\vec{p}_m^T\\vec{p}_{i} -\n",
    "  \\vec{p}_{j}^T 2\\lambda_m\\vec{p}_{m} \\\\\n",
    "  &= 0 + \\eta_j\\vec{p}_{j}^T \\vec{p}_{j}- 0 \\\\\n",
    "  & = \\eta_j\n",
    "\\end{align}\n",
    "for $j=1,\\ldots,m-1$. So the problem simplifies to\n",
    "$$0 = 2C\\vec{p}_m - 2\\lambda_m\\vec{p}_{m}$$\n",
    "from which we see that a critical point of the Lagrange equation has to fulfill\n",
    "$$C\\vec{p}_m =\\lambda_m\\vec{p}_{m}$$\n",
    "which just means it has to be an eigenvector of the matrix $C$ with eigenvalue $\\lambda_M$. There may be multiple eigenvectors for $C$, so we have to select $\\vec{p}_m$ in a way that it maximizes the variance in direction $\\vec{p}_m$, i.e. the value\n",
    "$$\\vec{p}_{m}^TC\\vec{p}_{m} = \\vec{p}_{m}^T\\lambda_M\\vec{p}_{m} = \\lambda_M.$$\n",
    "This just means that we have to choose $\\vec{p}_m$ to be the eigenvector with the largest eigenvalue (amongst those not previously selected). This completes the inductive step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
