{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OsnabrÃ¼ck University - Machine Learning (Summer Term 2016) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, June 26, 2016**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Classifiers and SVM [6 Points]\n",
    "\n",
    "In the lecture (ML-09 Slides 7ff) several types of classifiers have been introduced. In this assignment you will explore differences and similarities between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) LDA\n",
    "\n",
    "How does the LDA classifier work? What restrictions have to be fullfilled by the data for this method to work and why?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The LDA classifier assumes two normally distributed classes of data with the same covariance matrices each. It subtracts the mean of the two distribution from each other and normalizes by the inverse variance. \n",
    "Taking the inner product of the weight (the normalized difference function) with the new data point yields either a positive or a negative value which can be compared to the threshold to determine the estimator for the class of the new data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Nearest Neighbor\n",
    "\n",
    "How does the nearest neighbor classifier work? When would you use it and how is it trained?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For a new sample the nearest neighbor classifier searches for the corresponding nearest data point in our training data.\n",
    "You would use it when memory and runtime timings are not of great importance, as training is instant: You just store the training data. However, classification becomes a computationally more expensive task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Support Vector Machines\n",
    "\n",
    "Name some differences between a SVM and a MLP. When would you use which?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| SVM | MLP |\n",
    "|-----|-----|\n",
    "| linear separatrix (unless using kernel trick) | non-linear separatrix |\n",
    "| binary classifier (extension to cascade possible) | multiple classes possible |\n",
    "| no online training | online training possible |\n",
    "| can deal with outliers: slack variables | outliers affect the whole network |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Ultimate Dinosaur 3000 M4ze Xtrem!  [14 Points]\n",
    "\n",
    "In this assignment you will have a look at the Q-Learning algorithm described in the lecture (ML-10 Slide 18). For this we generate a field with random rewards. A learning agent is then exploring the field and learns the optimal path to navigate through it. The code below is again filled with some ``TODO``s that should be filled by you in order to implement the Q-Learning algorithm. \n",
    "\n",
    "Below you also find a free-code field for a complete own implementation. You may use your own test mazes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "\n",
    "def generate_field(x, y, num_rewards, max_reward):\n",
    "    \"\"\"\n",
    "    Generate a random game field with rewards.\n",
    "    \n",
    "    Args:\n",
    "        x            x dimension of the field\n",
    "        y            y dimension of the field \n",
    "        num_rewards  the number of rewards that should be randomly placed\n",
    "        max_reward   the maximum reward that can be placed \n",
    "        \n",
    "    Returns:\n",
    "        A field with randomly initialized rewards, the rest of the \n",
    "        entries is zero\n",
    "    \"\"\"\n",
    "    field = np.zeros((y,x), dtype=np.uint8)\n",
    "    \n",
    "    for i in range(num_rewards):\n",
    "        field[rand.randint(y), rand.randint(x)] = rand.choice(max_reward)\n",
    "    \n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class QLearning:\n",
    "    \"\"\"\n",
    "    This class contains all the necessary methods to navigate through\n",
    "    a maze or game with the help of a little bit of Q-Learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field, actions, gamma):\n",
    "        \"\"\"\n",
    "        Initializes the QLearning Algorithm with the necessary parameters.\n",
    "        All q values are stored in self.q - this is an array that has\n",
    "        ACTIONS x map_x x map_y dimensions to store a value for each action\n",
    "        in each field. The starting position self.pos is randomly initialized.\n",
    "        \n",
    "        Args:\n",
    "            field   the map\n",
    "            actions the available actions\n",
    "            gamma   the gamma in the lecture slides\n",
    "        \n",
    "        Returns:\n",
    "            An instance that can be used for QLearning on the field\n",
    "        \"\"\"\n",
    "        # q stores the q_values for each action in each space of the field.\n",
    "        self.field = field\n",
    "        self.actions = actions\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Remember the map extend for further navigation.\n",
    "        self.map_y = self.field.shape[0]\n",
    "        self.map_x = self.field.shape[1]\n",
    "        \n",
    "        # Create q value matrix.\n",
    "        self.q = np.zeros((len(self.actions), self.map_y, self.map_x))\n",
    "\n",
    "        # Start on a random position in the field.\n",
    "        self.pos = [np.random.randint(self.map_y), np.random.randint(self.map_x)]\n",
    "\n",
    "\n",
    "    def get_coordinates(self, position, choice):\n",
    "        \"\"\"\n",
    "        Returns the coordinates that follow a certain choice, depending\n",
    "        on the current position of the learner. If the border is reached\n",
    "        the agent just stops there.\n",
    "        \n",
    "        Args:\n",
    "            position the current position\n",
    "            choice   the action that should be performed (one of: 'up', 'down', ...)\n",
    "            \n",
    "        Returns:\n",
    "            the updated coordinates\n",
    "        \"\"\"\n",
    "        # TODO return the right new coordinates depending on the position\n",
    "        y_new = position[0]\n",
    "        x_new = position[1]\n",
    "\n",
    "        if choice == 'left':  \n",
    "            x_new -= 1 if x_new > 0 else 0\n",
    "        elif choice == 'right': \n",
    "            x_new += 1 if x_new < self.map_x - 1 else 0            \n",
    "        elif choice == 'up':    \n",
    "            y_new -= 1 if y_new > 0 else 0                \n",
    "        elif choice == 'down':  \n",
    "            y_new += 1 if y_new < self.map_y - 1 else 0\n",
    "        else: \n",
    "            raise ValueError('No such action: {}'.format(choice))\n",
    "\n",
    "        return (y_new, x_new)\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Implementation of the update step. Closely follows the Algorithm described on\n",
    "        ML-10 Sl.18. Note that the you have attributes available as specified in the\n",
    "        __init__ method of this class, in addition to that is the FIELD variable that\n",
    "        stores the real field the agent is iterating about, as well as ACTIONS which\n",
    "        stores the available actions.\n",
    "        \"\"\"\n",
    "        # TODO: Select a random action that should be performed next.\n",
    "        # Be careful to handle the case where you hit the wall!\n",
    "        choice = np.random.choice(ACTIONS)\n",
    "        choice_i = self.actions.index(choice)\n",
    "        new_pos = self.get_coordinates(self.pos, choice)\n",
    "        # Step out when we hit a wall.\n",
    "        if new_pos == self.pos: return\n",
    "\n",
    "        # TODO: Receive the reward for the new position from the field.\n",
    "        rew = self.field[new_pos[0], new_pos[1]]\n",
    "\n",
    "        # TODO: Update the q-value for the performed action.\n",
    "        self.q[choice_i, self.pos[0], self.pos[1]] = rew + self.gamma * max(self.q[:, new_pos[0], new_pos[1]])\n",
    "\n",
    "        # TODO: Update the position of the player to the new field.\n",
    "        self.pos = new_pos\n",
    "\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Plots the current state.\n",
    "        \"\"\"\n",
    "        fig_player = plt.figure('QLearning State')\n",
    "\n",
    "        for i, direc in enumerate(ACTIONS):\n",
    "            plt.subplot(3,3,2*i+2)\n",
    "            plt.axis('off')\n",
    "            plt.title(direc)\n",
    "            plt.imshow(self.q[i,:,:], interpolation = 'None')\n",
    "\n",
    "        fig_player.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Determine the size of the field, change this parameters as you like\n",
    "m_x = 5\n",
    "m_y = 4\n",
    "\n",
    "steps = 200\n",
    "\n",
    "ACTIONS = ['up','left','right','down']  # Those are the availabe actions for the QLearning.\n",
    "FIELD = generate_field(m_x, m_y, 5, 10) # The field that is used for learning.\n",
    "\n",
    "# Plotting the generated field\n",
    "figure = plt.figure('Field')\n",
    "plt.axis('off')\n",
    "plt.imshow(FIELD, interpolation='none')\n",
    "figure.canvas.draw()\n",
    "\n",
    "# TODO: Generate a QLearning instance with the right parameters.\n",
    "player = QLearning(FIELD, ACTIONS, 0.9)\n",
    "\n",
    "# Now we perform steps many learning iterations on the field with\n",
    "# the generated QLearning instance.\n",
    "for i in range(steps):     \n",
    "    player.update()\n",
    "    player.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are also free to write your complete own implementation of the QLearning algorithm. Use the following cell for your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code goes here:\n",
    "%matplotlib notebook\n",
    "\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "maze = np.array([[0, 0, .5, 0, 0, 0, 0, 0, .5], \n",
    "                 [0, 0, 0, .2, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, .8, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 1, 0, 0, 0, 0, 0]])\n",
    "actions = [lambda c : (c[0], c[1] + 1),\n",
    "           lambda c : (c[0], c[1] - 1),\n",
    "           lambda c : (c[0] - 1, c[1]),\n",
    "           lambda c : (c[0] + 1, c[1])]\n",
    "\n",
    "plt.figure('Q Learning')\n",
    "plt.subplot(121).imshow(maze, interpolation='none', cmap='hot')\n",
    "\n",
    "def move(pos, direction):\n",
    "    new_pos = actions[direction](pos)\n",
    "    for dim, c in enumerate(new_pos):\n",
    "        if c < 0 or c >= maze.shape[dim]:\n",
    "            raise ValueError('Action impossible.')\n",
    "    return new_pos\n",
    "\n",
    "# (Initialize parameters)\n",
    "gamma = 0.9\n",
    "\n",
    "# Initialize q(s, a) <- 0\n",
    "q = np.zeros((np.prod(maze.shape), len(actions)))\n",
    "\n",
    "for run in range(100):\n",
    "    # Observe current state s\n",
    "    position = (0, 0)\n",
    "    s = np.ravel_multi_index(position, maze.shape)\n",
    "\n",
    "    # Repeat\n",
    "    for iteration in range(100):\n",
    "        # Select action a\n",
    "        a = np.random.randint(len(actions))\n",
    "        # Execute action a (if possible)\n",
    "        try:\n",
    "            position = move(position, a)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        # Receive reward r\n",
    "        r = maze[position]\n",
    "        # Observe new state s_n\n",
    "        s_n = np.ravel_multi_index(position, maze.shape)\n",
    "\n",
    "        # Update q(s, a)\n",
    "        q[s, a] = r + gamma * np.max(q[s_n, :])\n",
    "\n",
    "        # Update s\n",
    "        s = s_n\n",
    "    plt.subplot(122).imshow(q, interpolation='none', cmap='hot')\n",
    "    plt.gcf().canvas.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
