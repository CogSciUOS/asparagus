{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OsnabrÃ¼ck University - Machine Learning (Summer Term 2016) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 04: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, May 8, 2016**. If you need help (and Google and other resources were not enough), feel free to contact your groups designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder.\n",
    "\n",
    "In the following tasks we will be relying on numpy. Using the following import we expect it to be in global scope as `np`. Therefore we can, after executing the following cell, use stuff like `np.array` and `np.sqrt`. Check out the [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/index.html) and especially search it using e.g. [Google Site Search](https://www.google.de/search?q=array+site%3Adocs.scipy.org%2Fdoc%2Fnumpy)! You can also try `np.lookfor('keyword search docstrings')` to get help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.lookfor('get array diagonal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Distance Measures [4 Points]\n",
    "\n",
    "This exercise is focused on implementing some distance functions that later on help with the clustering. They should all use the euclidean distance as a measurement. The first function is closely related to the MATLAB pdist2 function. This is the summary from the [documentation](http://de.mathworks.com/help/stats/pdist2.html):\n",
    "\n",
    "*D = pdist2(X,Y) returns a matrix D containing the Euclidean distances between each pair of observations in the mx-by-n data matrix X and my-by-n data matrix Y. Rows of X and Y correspond to observations, columns correspond to variables. D is an mx-by-my matrix, with the (i,j) entry equal to distance between observation i in X and observation j in Y. The (i,j) entry will be NaN if observation i in X or observation j in Y contain NaNs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pdist2(X, Y):\n",
    "    \"\"\"\n",
    "    Pairwise distance between all points of two datasets.\n",
    "    X and Y are expected to be numpy arrays of size mx-by-n and my-by-n, respectivley. \n",
    "    n and m being the amount of observations in the first dimension of each set.\n",
    "    \"\"\"\n",
    "    dist = np.ndarray((X.shape[0], Y.shape[0]))\n",
    "    for xi, xv in enumerate(X):\n",
    "        for yi, yv in enumerate(Y):\n",
    "            dist[xi][yi] = np.sqrt(np.sum((xv - yv) ** 2))\n",
    "    return dist\n",
    "\n",
    "\n",
    "X = np.array([[1,2,3], [4,5,6], [7,8,9], [10,11,12]])\n",
    "Y = np.array([[13,14,15], [16,17,18], [19,20,21]])\n",
    "N = np.array([[1,2,3], [float('NaN'),1,0]])\n",
    "\n",
    "assert pdist2(X, Y).shape == (4, 3), \"Shape is wrong: {}\".format(pdist2(X, Y).shape)\n",
    "assert np.round(pdist2(X, Y)[3,0], 3) == 5.196, \"[10,11,12] and [13,14,15] is wrong: {}\".format(pdist2(X, Y)[3,0])\n",
    "assert np.round(np.mean(pdist2(X, Y)), 3) == 18.187, \"Mean distance is wrong: {}\".format(np.mean(pdist2(X, Y)))\n",
    "assert np.isnan(pdist2(X, N)[0,1]), \"Should be NaN: {}\".format(pdist2(X, N)[0,1])\n",
    "\n",
    "del X, Y, N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the $d_{mean}$ and $d_{centroid}$ distance from the lecture. Each function expects two clusters each represented by a 2-dimensional numpy array, where the number of columns $n$ reflects the dimensionality of the data space and has to agree for both clusters, while the number of rows $mx$ and $my$ can vary from cluster to cluster. The return value is the respective distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def d_mean(X, Y):\n",
    "    \"\"\"\n",
    "    Mean distance between points of two clusters.\n",
    "    X and Y are expected to be numpy arrays.\n",
    "    \"\"\"\n",
    "    return np.mean(pdist2(X, Y))\n",
    "\n",
    "\n",
    "X = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "Y = np.array([[13,14,15], [16,17,18], [19,20,21], [5,45,1], [1,12,7]])\n",
    "\n",
    "assert np.round(d_mean(X,Y), 3) == 22.297, \"Result is not correct: {}\".format(d_mean(X, Y))\n",
    "assert d_mean(X, Y) == d_mean(Y, X), \"X,Y is not equal to Y,X: {} != {}\".format(d_mean(X, Y), d_mean(Y, X))\n",
    "\n",
    "del X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def d_centroid(X, Y):\n",
    "    \"\"\"\n",
    "    Distance between the centroids of two clusters.\n",
    "    X and Y are expected to be numpy arrays.\n",
    "    \"\"\"\n",
    "    cent_X = sum(X) / len(X)\n",
    "    cent_Y = sum(Y) / len(Y)\n",
    "    \n",
    "    return pdist2(np.array([cent_X]), np.array([cent_Y]))[0][0]\n",
    "\n",
    "\n",
    "X = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "Y = np.array([[13,14,15], [16,17,18], [19,20,21]])\n",
    "Z = np.array([[-2,0], [-1,100]])\n",
    "W = np.array([[2,0], [1,100], [1,-100], [1,-20]])\n",
    "\n",
    "assert np.round(d_centroid(X, Y), 3) == 20.785, \"Result is not correct: {}\".format(d_centroid(X, Y))\n",
    "assert np.round(d_centroid(Z, W), 3) == 55.069, \"Result is not correct: {}\".format(d_centroid(Z, W))\n",
    "assert d_centroid(X, Y) == d_centroid(Y, X), \"X,Y is not equal to Y,X: {} != {}\".format(d_centroid(X, Y), d_centroid(Y, X)) \n",
    "\n",
    "del X, Y, Z, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Hierarchical Clustering [2 Points]\n",
    "\n",
    "In the following you find implementations for single- and complete-linkage clustering. *This implementation relies on the distance functions you wrote in Assignment 1 (if you get stuck on the first exercise write an email to the tutors and we will help you along).* Take a look at the code (this might also help if you get stuck on assignment 3) and answer the question posted below. You may of course change parameters and try it out on different datasets (`points.txt` & `clusterData.txt` are provided).\n",
    "\n",
    "Note that for performance reasons the code differs from the lecture's pseudocode (ML-05 Slide 8), but in general it does the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linkage(data, k=5, complete=False):\n",
    "    \"\"\"\n",
    "    Runs single or complete linkage clustering.\n",
    "    \"\"\"\n",
    "    # Initially all points are their own cluster.\n",
    "    labels = np.arange(len(data))\n",
    "\n",
    "    # Calculate distance between all points.\n",
    "    # Also removing half of the matrix because \n",
    "    # its symmetrical along the diagonal.\n",
    "    dst = np.tril(pdist2(data, data))\n",
    "\n",
    "    while len(set(labels)) > k:\n",
    "        # Get the lowest distance of two points which\n",
    "        # do not have the same label.\n",
    "        r,c = np.where(dst==np.min(dst[dst>0]))\n",
    "        \n",
    "        # Ignore the case when there are multiple with\n",
    "        # equally smallest distance.\n",
    "        r = r[0]\n",
    "        c = c[0]\n",
    "\n",
    "        # The two points are now in the same cluster,\n",
    "        # so they have a distance of 0 now.\n",
    "        dst[r,c] = 0\n",
    "\n",
    "        # Make the two clusters have the same label.\n",
    "        labels[labels==labels[r]] = labels[c]\n",
    "\n",
    "        # Check if we want to do complete linkage clustering.\n",
    "        if complete:\n",
    "            # Update the distances of the points which are not in the same cluster.\n",
    "            for i in np.nonzero(dst[r,:]>0)[0]:\n",
    "                dst[r,i] = np.max(pdist2(data[i,np.newaxis], data[labels==labels[r],:]))\n",
    "\n",
    "            # The distances to c are now the same as to r, so we can just\n",
    "            # set them to zero - would be duplicates otherwise.\n",
    "            dst[:,c] = 0\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the data.\n",
    "data = np.loadtxt('points.txt')\n",
    "\n",
    "# Show unprocessed data set.\n",
    "fig_cluster = plt.figure('Unprocessed Cluster Data')\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "fig_cluster.canvas.draw()\n",
    "\n",
    "# Apply Single Linkage Clustering\n",
    "labels = linkage(data, k=5, complete=False)\n",
    "fig_single = plt.figure('Single-linkage Clustering with k=5')\n",
    "plt.scatter(data[:,0], data[:,1], c=labels)\n",
    "fig_single.canvas.draw()\n",
    "\n",
    "# Apply Complete Linkage Clustering\n",
    "labels = linkage(data, k=5, complete=True)\n",
    "fig_complete = plt.figure('Complete-linkage Clustering with k=5')\n",
    "plt.scatter(data[:,0], data[:,1], c=labels)\n",
    "fig_complete.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between single- and complete-linkage clustering and which is the better solution given the dataset?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Single-linkage tends to chain clusters along the data. That is why it combines the points in the center area with those in the bottom right corner.\n",
    "\n",
    "Complete-linkage prefers compact clusters and thus combines each of the point heavy areas individually without merging them.\n",
    "\n",
    "For this dataset, complete-linkage is superior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: k-means Clustering [8 Points]\n",
    "\n",
    "Implement k-means clustering. Plot the results for $k = 7$ and $k = 3$ in colorful scatter plots.\n",
    "\n",
    "How could one handle situations when one or more clusters end up containing 0 elements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeans(data, k = 3):\n",
    "    \"\"\"\n",
    "    Applies kmeans clustering to the data using k initial clusters.\n",
    "    data is expected to be a numpy array of size n*2, \n",
    "    n being the amount of observations in the data. This function returns\n",
    "    the centroids and the labels for the clusters data (1,1,3,5,5,5,...)\n",
    "    \"\"\"\n",
    "    # Initial centroids are k random samples from the data.\n",
    "    centroids = data[np.random.randint(0, data.shape[0], k)]\n",
    "    old_centroids = np.zeros(centroids.shape)\n",
    "    \n",
    "    # Initial labels are all.. something.\n",
    "    labels = np.ndarray(data.shape[0])\n",
    "    \n",
    "    # Lets keep count of our iterations to avoid infinite loops.\n",
    "    iterations = 0\n",
    "    \n",
    "    while np.any(np.abs(centroids - old_centroids) > np.finfo(float).eps) and iterations < 1000:\n",
    "        # Keep count of iterations and remember current centroids for change calculation.\n",
    "        iterations = iterations + 1\n",
    "        #copy the centroids and keep them for break condition check\n",
    "        old_centroids = np.copy(centroids)\n",
    "        \n",
    "        # Calculate new labels. Labels are the index of their minimal distance to any centroid.\n",
    "        labels = np.argmin(pdist2(centroids, data), axis=0)\n",
    "        \n",
    "        # Update centroids using the new cluster labels.\n",
    "        for label in range(k): \n",
    "            # Check for empty clusters.\n",
    "            if (len(labels == label) > 0):\n",
    "                # Cluster is not empty, move its centroid to new mean.\n",
    "                centroids[label, :] = np.mean(data[labels == label], axis=0)\n",
    "            else:\n",
    "                # Cluster is empty, set its centroid to the furthest outlier.\n",
    "                blacksheep = np.argmax(pdist2(centroids, data), axis=0)\n",
    "                centroids[label, :] = data[blacksheep, :]\n",
    "\n",
    "    # Return labels and centroids for pretty plotting.\n",
    "    return (labels, centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('clusterData.txt')\n",
    "\n",
    "for k in [3, 7]:\n",
    "    labels, centroids = kmeans(data, k)\n",
    "\n",
    "    kmeans_fig = plt.figure('k-means with k={}'.format(k))\n",
    "    plt.scatter(data[:,0], data[:,1], c=labels)\n",
    "    plt.scatter(centroids[:,0], centroids[:,1], \n",
    "                c=list(set(labels)), alpha=.1, marker='o',\n",
    "                s=np.array([len(labels[labels==label]) for label in set(labels)])*100)\n",
    "    kmeans_fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4: Soft Clustering with Gaussian Mixture [6 Points]\n",
    "\n",
    "In this assignment you will calculate the update rules for a Gaussian Mixture model required for the M-step of the EM algorithm. The Gaussian Mixture model can be used for soft clustering since it allows us to express varying degrees of certainty about the membership of individual samples. It is one of the most widely used models since Gaussian distributions generally have the property of fitting all different kinds of data reasonably well.\n",
    "\n",
    "A mixture model with $K$ components is in general of the form:\n",
    "\n",
    "$$ p(\\mathbf{x}|\\mathbf{\\theta}) = \\sum_{k=1}^K\\pi_kp_k(\\mathbf{x}|\\mathbf{\\theta}_k)$$\n",
    "where $\\sum_{k=1}^K\\pi_k = 1$.\n",
    "\n",
    "This means that the probability of observing a dataset $\\mathbf{x}$ given the parameter vector $\\mathbf{\\theta}$ can be expressed as the sum of $K$ individual distributions $p_k$ with parameters $\\mathbf{\\theta}_k \\subseteq {\\theta}$ which are weighted by respective class probabilities $\\pi_k$. The probability for individual data $x_i \\in \\textbf{x}$ is calculated correspondingly (note however, that of course the values for individual data differs from the overall probability). \n",
    "\n",
    "We can now choose distributions for $p_k$ and $\\pi_k$ and we get a whole collection full of different possible models, each of which has its own advantages and disadvantages (you can check <a href='https://en.wikipedia.org/wiki/Mixture_model'>Wikipedia</a> if you want an overview). The easiest case is where our mixing distributions are normally distributed, $p_k \\sim \\mathcal{N}(\\mu_k,\\sigma_k)$, and our latent class probabilities have a discrete distribution where we only have $\\pi_k \\in [0,1]$ and the constraint $\\sum_k\\pi_k=1$.\n",
    "\n",
    "If we were to randomly pick values for the parameter vector $\\theta$ then we would now have a generative model that can produce naturally clustered data for us, we would just have to sample $\\hat{x} \\sim p(\\mathbf{x}|\\mathbf{\\theta})$. However, we want to go into the opposite direction and figure out what the distribution of the labels given the data is. This can be calculated easily by Bayes' Theorem for each model $k$, where the (latent) probability for choosing model $k$ is $p(k|\\mathbf{\\theta}_k) = \\pi_k$:\n",
    "\n",
    "$$p(k|\\mathbf{x},\\mathbf{\\theta})=\\frac{p(k|\\mathbf{\\theta}_k)p(\\mathbf{x}|k,\\mathbf{\\theta}_k)}{\\sum_{k'=1}^Kp(k'|\\mathbf{\\theta}_{k'})p(\\mathbf{x}|k',\\mathbf{\\theta}_{k'})} = \\frac{\\pi_kp_k(\\mathbf{x}|\\mathbf{\\theta}_k)}{\\sum_{k'=1}^K\\pi_{k'}p_{k'}(\\mathbf{x}|\\mathbf{\\theta}_{k'})}$$\n",
    "\n",
    "That sounds good enough, but where do we actually start now? We have a mathematical framework pinned down, but it contains many variables and it is not *a priori* obvious how we can figure out the best values for them. We *have* some data which we want to use to determine the parameters so the usual approach would be to simply calculate a Maximum Likelihood Estimator (MLE) or an Maximum A Posteriori Estimator (MAP) by maximizing the above formulas over the possible parameters with a method like Gradient Descent. It turns out however that this is very, very hard to do (optimal MLE for a GMM is NP-hard (Aloise et al. 2009; Drineas et al. 2004)) since the $\\pi_k$ and the $\\theta_k$ are strongly interdependent and neither is known. It *can* still be done with some work-arounds, but there is also an alternative path that we can go down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(The following exhibition is only for those who are interested in the mathematical background of the EM-algorithm, those who only want to solve the exercise can skip ahead to the function that you have to maximize.)*\n",
    "\n",
    "We want to maximize the log likelihood given as\n",
    "$$\\mathcal{\\ell}(\\mathbf{\\theta})=\\sum_{i=1}^N\\log p(x_i|\\mathbf{\\theta}) = \\sum_{i=1}^N\\log\\left[\\sum_{k=1}^Kp_k(x_i|\\mathbf{\\theta}_k)\\right].$$\n",
    "All the problems occur because we have a sum inside the logarithm and so we can't pull the logarithm further in towards the densitiy and that is what makes the problem so hard. If we just *ignore* the inner sum we get an expression\n",
    "$$\\mathcal{\\ell}_c(\\mathbf{\\theta}) = \\sum_{i=1}^N\\log p_k(x_i|\\mathbf{\\theta}_k)$$\n",
    "which would be much nicer to compute. But now we have a free floating $k$ in the subscript of our density! Which one of the mixing distributions are we talking about here? Kind of all of them at once. But we need one quantity to represent all the distributions. So to get rid of the $k$ we take the expected value with respect to the latent variable $k$ and receive a function that only depends on $\\mathbf{\\theta}$:\n",
    "$$Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) = \\mathbb{E}\\left[\\mathcal{\\ell}_c\\left(\\mathbf{\\theta}\\middle|\\mathcal{\\theta}^{t-1}\\right)\\right]$$\n",
    "\n",
    "Calculating this $Q$ function can be difficult - but at least we only have to do it once instead of solving an NP-hard optimization problem every time we have a new dataset. We will only provide you the final formula, you will have to trust us on this one:\n",
    "\n",
    "$$\\begin{align}\n",
    "Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) &= \\sum_i\\sum_k p\\left(k\\middle|x_i,\\mathbf{\\theta}^{t-1}\\right)\\log\\pi_k + \\sum_i\\sum_k p\\left(k\\middle|x_i,\\mathbf{\\theta}^{t-1}\\right)\\log p_k\\left(x_i\\middle|\\mathbf{\\theta}\\right)\n",
    "\\end{align}$$\n",
    "\n",
    "This still looks nasty but it really isn't that bad! Since $\\theta^{t-1}$ is known at time $t$ we can calculate $p\\left(k\\middle|x_i,\\mathbf{\\theta}^{t-1}\\right)$ with Bayes' Theorem as stated above and replace these expressions with constants $r_{i,k}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is where your work begins:**\n",
    "\n",
    "$\\DeclareMathOperator*{\\argmax}{arg\\,max}$\n",
    "In the lecture you saw a proof that if we choose\n",
    "$$\\mathbf{\\theta}^t = \\argmax_{\\mathbf{\\theta}} Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right)$$\n",
    "that the likelihood of the parameter is non-decreasing then. So we want to maximize $Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right)$ for the parameters $\\left(\\pi_1\\dots,\\pi_K\\right)$ and $\\theta = \\left(\\mu_1,\\dots,\\mu_K,\\sigma_1,\\dots,\\sigma_K\\right)$. So your job is to take the derivative of \n",
    "$$\\begin{align}\n",
    "Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) &= \\sum_i\\sum_k r_{i,k}\\log\\pi_k + \\sum_i\\sum_k r_{i,k}\\log p_k\\left(x_i\\middle|\\mathbf{\\theta}\\right)\n",
    "\\end{align}$$\n",
    "with respect to these variables, to set it equal to 0 and to solve for the value that you are currently maximizing for. You only have to do this for the one dimensional case, i.e. \n",
    "$$p_k(x_i|\\mathbf{\\theta}_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\\exp\\left({-\\frac{\\left(x_i-\\mu_k\\right)^2}{2\\sigma_k^2}}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Calculate the maximizer for the $\\pi_k$ (You need the ensure $\\sum_k\\pi_k =1$. You can either use a Lagrangian Multiplier for this or use the formula to express one of the $\\pi_i$ in terms of all the others):**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align} \n",
    "\\frac{\\partial}{\\partial \\pi_k}\\left(Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) - \\lambda \\left(\\sum_k \\pi_k - 1\\right)\\right) &= \\sum_i \\frac{r_{i,k}}{\\pi_k} - \\lambda = 0 \\Leftrightarrow \\sum_i \\frac{r_{i,k}}{\\lambda} = \\pi_k \\\\\n",
    "\\frac{\\partial}{\\partial \\lambda}Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) + \\lambda \\left(\\sum_k \\pi_k - 1\\right) &= \\sum_k \\pi_k - 1 \\stackrel{!}{=} 0 \\Leftrightarrow \\sum_k \\pi_k = 1 \\\\\n",
    "\\Rightarrow \\frac{1}{\\lambda}\\sum_k\\sum_i r_{i,k} &= 1 \\\\\n",
    "\\Rightarrow \\pi_k &= \\frac{1}{N}\\sum_i r_{i,k}\n",
    "\\end{align}$$\n",
    "\n",
    "Because of popular request: If we don't want to use the Lagrangian the notation becomes a bit more cumbersome but the overall strategy remains the same.\n",
    "\n",
    "$$ \\begin{align}\n",
    "\\sum_{k=1}^K\\pi_k = 1 &\\Leftrightarrow \\pi_K = 1 - \\sum_{k=1}^{K-1}\\pi_k \\\\\n",
    "\\frac{\\partial}{\\partial \\pi_k}Q &= \\frac{\\partial}{\\partial \\pi_k}\\sum_{i=1}^N \\left(\\sum_{k=1}^{K-1}r_{ik}\\log\\pi_k + r_{ik}\\log(1-\\sum_{k=1}^{K-1}\\pi_k)\\right) \\\\\n",
    "&= \\sum_{i=1}^N\\left(\\frac{r_{ik}}{\\pi_k} - \\frac{r_{ik}}{1-\\sum_{k=1}^{K-1}\\pi_k}\\right) \\stackrel{!}{=} 0 \\\\\n",
    "\\Leftrightarrow \\pi_k &= \\left(\\sum_{i=1}^{N}r_{ik}\\right)\\frac{\\left(1 - \\sum_{k=1}^{K-1}\\pi_k\\right)}{\\sum_{i-1}^Nr_{iK}} \\\\\n",
    "\\stackrel{\\text{sum over k}}{\\Rightarrow} \\frac{\\left(1 - \\sum_{k=1}^{K-1}\\pi_k\\right)}{\\sum_{i-1}^Nr_{iK}} \\sum_{k=1}^K\\sum_{i=1}^Nr_{ik} &= 1 \\\\\n",
    "\\Leftrightarrow \\frac{\\left(1 - \\sum_{k=1}^{K-1}\\pi_k\\right)}{\\sum_{i-1}^Nr_{iK}} &= \\frac{1}{N}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Calculate the maximizer for the $\\mu_k$:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align} \n",
    "\\frac{\\partial}{\\partial \\mu_k}Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) &= \\frac{\\partial}{\\partial \\mu_k}\\sum_i\\sum_k r_{i,k}\\log p_k(\\mathbf{x}_i|\\mathbf{\\theta}) \\\\\n",
    "&= \\sum_i r_{i,k} \\frac{\\partial}{\\partial \\mu_k} \\left(-\\frac{1}{2}\\log\\left(\\sigma_k^2\\right) - \\frac{1}{2\\sigma_k^2}\\left(x_i-\\mu_k\\right)^2\\right)\n",
    "\\\\\n",
    "&= \\frac{1}{\\sigma_k^2}\\sum_i r_{i,k}(x_i-\\mu_k) \\stackrel{!}{=} 0 \\\\\n",
    "\\Leftrightarrow \\sum_i r_{i,k}x_i &= \\mu_k\\sum_ir_{i,k} \\\\\n",
    "\\Leftrightarrow \\mu_k &= \\frac{\\sum_i r_{i,k}x_i}{\\sum_ir_{i,k}} \n",
    "\\end{align} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Calculate the maximizer for the $\\sigma_k^2$:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\sigma_k^2}Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) &= \\sum_i r_{i,k} \\frac{\\partial}{\\partial \\sigma_k^2} \\left(-\\frac{1}{2}\\log\\left(\\sigma_k^2\\right) - \\frac{1}{2\\sigma_k^2}\\left(x_i-\\mu_k\\right)^2\\right) \\\\\n",
    "&= \\sum_i r_{i,k}\\left(\\frac{1}{\\sigma_k^4}(x_i-\\mu_k)^2 -\\frac{1}{\\sigma_k^2}\\right) \\stackrel{!}{=} 0\\\\\n",
    "\\Leftrightarrow \\sum_i r_{i,k}(x_i-\\mu_k)^2 &= \\sigma_k^2\\sum_ir_{i,k} \\\\\n",
    "\\Leftrightarrow \\sigma_k^2 &= \\frac{\\sum_i r_{i,k}(x_i-\\mu_k)^2}{\\sum_ir_{i,k}}\n",
    "\\end{align}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
