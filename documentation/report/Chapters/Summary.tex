\section{Summary of results}
\label{ch:Summary}

The current work was conducted to investigate the state of the art on asparagus classification aiming at the development of approaches that could help to improve the current sorting algorithm implemented at the Autoselect ATS~II at the asparagus farm Gut Holsterfeld. Data was collected, preprocessed and then analyzed with seven different approaches. Out of our 591495 images corresponding to 197165 different asparagus spears, 13271 were collected by re-sorting in the machine and 13319 images manually labeled by us. This labeled data was considered for the supervised approaches. The semi-supervised approach was in addition based on approximately equally many unlabeled images. The results illustrate that classifying asparagus is not a trivial problem. However, the results also show that it is possible to extract relevant features and to improve current sorting approaches.

\bigskip
For supervised learning, we employed \acrshortpl{mlp} and  \acrshortpl{cnn}. Whereas the earlier were trained on sparse descriptions retrieved by high level feature engineering, the latter were directly trained on preprocessed images. They include a dedicated network for head-related features, as well as networks for single-label classification and multi-label classification. All approaches aim to solve the same image classification problem, using supervised learning.

The feature engineering \acrshort{mlp} for curvature prediction and the single-label classification \acrshort{cnn} perform binary classification (see \autoref{subsec:FeatureEngineering} and \autoref{subsec:SingleLabel}), whereas the  \acrshort{cnn} for head-related features as well as the multi-label approach perform multi-label classification (see~\autoref{subsec:HeadNetwork} and~\autoref{subsec:MultiLabel}).

With the random forest approach, a third option namely multiclass prediction is added that works completely differently. Each approach has drawbacks and benefits. The complexity and the requirement to specify many parameters has proven to be a disadvantage of relatively deep, but also rather shallow \acrshortpl{cnn} as compared to e.g. \acrshortpl{mlp} or random forests. In contrast, stronger preprocessing or even feature engineering is required to successfully employ the latter. After all, however, the most important criterion to evaluate an approach is its predictive performance.

The heterogeneity of approaches with respect to the number of target categories and the variety of performance measures pose challenges for a direct comparison using the overall accuracies. Therefore, feature-wise evaluation appears most promising. As the distribution of some features (e.g. violet) has proven to be very unbalanced in our dataset, even high accuracies might relate to poor predictions (e.g. when the feature is never detected). Hence, feature-wise accuracies are only a coarse indicator of the modelâ€™s performance that may nonetheless give insights where difficulties lie and what features are more problematic than others. However, for some promising approaches we computed the sensitivity and specificity per feature to reveal a more fine-grained picture of the predictive performance.

\bigskip
The multiclass approach has an overall accuracy of 75\%. The performance of the  \acrshort{cnn} for head-related features is indicated by sensitivity and specificity values. Flower detection reaches 55\% sensitivity and 95\% specificity while rusty head detection attains only 19\% sensitivity at 98\% specificity. The overall accuracy of the multi-label \acrshort{cnn} approach reaches up to 87\%. For this model, accuracies are not calculated per feature.

In contrast, feature-wise accuracies for binary classification can be reported. The same holds for feature-wise performance measures that were calculated for some of the other approaches. The feature engineering based approaches show good results on curvature (82\% sensitivity, 67\% specificity) and similarly for violet detection (62\% sensitivity and 96\% specificity) as well as for rusty body (71\% sensitivity, 65\% specificity).

In the single-label \acrshort{cnn}, best results are achieved for features relying on the thickness and length of the asparagus. All of these features achieve a balanced accuracy above 90\%, with best results for the feature very thick (98\% sensitivity and 99\% specificity). Of the solely hand-labeled features, feature hollow shows the best performance (77\% sensitivity and 98\% specificity). The feature rusty head has the least performance (52\% sensitivity and 81\% specificity).

\bigskip
The unsupervised learning approaches, namely \acrshort{pca} and the convolutional autoencoder, both deal with dimension reduction. Both were trained on the sample set for which labels are available. While the classification method based on \acrshort{pca} targets at binary feature prediction (absence or presence of a feature), the unsupervised autoencoder does not predict labels. The accuracy of \acrshort{pca} is promising for length and hollow (100\%) but extremely poor for curvature detection (20\%). It has to be mentioned, however, that only very few samples were used for training and evaluation of the named approach. As such it is yet to be proven whether or not these results generalize.

\bigskip
In the last approach, a semi-supervised learning method was based on a partially labeled data set. It performs multi-label classification. Unfortunately, the predicted power was rather poor, best for curvature (18\% sensitivity, 96\% specificity) and worst for violet as it does not detect any violet spears (0\% sensitivity).