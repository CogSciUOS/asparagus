\section{Summary of results}
\label{ch:Summary}

The study project was conducted to investigate the state of the art of asparagus classification aiming at the development of approaches that could help to improve the current sorting algorithm implemented in the Autoselect ATS II at the asparagus farm Gut Holsterfeld. Data was collected, preprocessed and then analyzed with seven different approaches. Out of our 591495 images, roughly corresponding to 197165 different asparagus spears, 13271 images were collected by re-sorting with the machine and 13319 images were manually labeled by us. This labeled data is considered for the supervised approaches. The semi-supervised approach is in addition based on approximately equally many unlabeled images, 20000 in total. The unsupervised learning approaches are based on roughly 5500 images. The results illustrate that classifying asparagus is not a trivial problem. However, the results also show that it is possible to extract relevant features that might improve current sorting approaches.

\bigskip
For supervised learning, we employed \acrshortpl{mlp} and  \acrshortpl{cnn}. Whereas the former were trained on sparse descriptions retrieved by high level feature engineering, the latter were directly trained on preprocessed images. They include networks for single-label classification as well as multi-label classification. 
The feature engineering \acrshort{mlp} for curvature prediction and the single-label  \acrshort{cnn} perform binary classification, whereas the  \acrshort{cnn} for the multi-label approach as well as head-related features network perform multi-label classification. All approaches aim to solve the same image classification problem, using supervised learning.

Each approach has drawbacks and benefits. The complexity and the requirement to specify many parameters has proven to be a disadvantage of relatively deep but also rather shallow \acrshortpl{cnn} as compared to e.g.\ \acrshortpl{mlp}. In contrast, stronger preprocessing or even feature engineering is required to successfully employ the latter. After all, however, the most important criterion to evaluate an approach is its predictive performance.

The heterogeneity of approaches with respect to the number of target categories and the variety of performance measures pose challenges for a direct comparison using the overall accuracies. Therefore, feature-wise evaluation appears most promising. As the distribution of some features (e.g.\ violet) has proven to be very unbalanced in our data set, even high accuracies might relate to poor predictions (e.g.\ when the feature is never detected). Hence, feature-wise accuracies are only a coarse indicator of the modelâ€™s performance that may nonetheless give insights where difficulties lie and what features are more difficult to determine than others. However, for some promising approaches we computed the sensitivity and specificity per feature to reveal a more fine-grained picture of the predictive performance.

\bigskip
In the single-label \acrshort{cnn}, very good results are achieved for features relying on the thickness and length of the asparagus (see \autoref{subsec:SingleLabel}). All of these features achieve a balanced accuracy above 90\%, with best results for the feature very thick (98\% sensitivity and 99\% specificity). Of the solely hand-labeled features, feature hollow shows the best performance (77\% sensitivity and 98\% specificity). The feature rusty head has the least performance (52\% sensitivity and 81\% specificity).

The multi-label approach has an overall accuracy of 75\% (see \autoref{subsec:MultiLabel}). The performance of the  \acrshort{cnn} for the two head-related features is indicated by sensitivity and specificity values. Flower detection reaches 55\% sensitivity and 95\% specificity while rusty head detection attains only 19\% sensitivity at 98\% specificity. The overall accuracy of the multi-label \acrshort{cnn} approach reaches up to 87\%. For this model, accuracies are not calculated per feature.

In contrast, feature-wise accuracies for binary classification can be reported (see \autoref{subsec:FeatureEngineering}). The same holds for feature-wise performance measures that were calculated for some of the other approaches. The feature engineering based approaches show good results on all of its three detected features, namely for bent (82\% sensitivity, 67\% specificity) and similarly for violet detection (62\% sensitivity and 96\% specificity) as well as for rusty body (71\% sensitivity, 65\% specificity).

\bigskip
The unsupervised learning approaches, namely \acrshort{pca} and the convolutional autoencoder, both deal with dimension reduction. Both were trained on the sample set for which labels are available. While the classification method based on \acrshort{pca} targets at binary feature prediction (absence or presence of a feature) (see \autoref{subsec:PCA}), the unsupervised autoencoder does not predict labels (see \autoref{subsec:Autoencoder}). The accuracy of \acrshort{pca} is promising for length and hollow (100\%) but extremely poor for bent detection (20\%). It has to be mentioned that only very few samples were used for training and evaluation of the named approach. As such it is yet to be proven whether or not these results generalize.

\bigskip
A semi-supervised learning method was based on a partially labeled data set (see \autoref{subsec:VariationalAutoencoder}). A semi-supervised autoencoder and a semi-supervised variational autoencoder perform multi-label classification. The more simple semi-supervised autoencoder performs better. Unfortunately, the predicted power is still rather poor, best for fractured (57\% sensitivity, 100\% specificity) and worst for violet as it does not detect any violet spears (0\% sensitivity).

\bigskip
In the last approach, a random forest model that predicts the class labels based on the annotated features instead of features as the aforementioned approaches, delivers an average accuracy of about 75\%. Our analysis shows that the model recalls some class labels like I A Anna or Hohle more reliably than class labels like II A or II B.